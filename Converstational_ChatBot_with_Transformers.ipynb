{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Converstational ChatBot with Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hYGRGGelg7zJ",
        "uzdcCDVciGyR",
        "4g4fXGjplapO",
        "Bj5NlAsml7if",
        "OLyoB2vrmB3j"
      ],
      "authorship_tag": "ABX9TyMr6Zd3ThOFFSt6VJt7ap+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashe/NLP/blob/main/Converstational_ChatBot_with_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6SOVSxoTqAL"
      },
      "source": [
        "In this notebook, we will train a converstational agent(Chatbot) using [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) using the Transformer mentioned in the [Attention is all you Need](https://arxiv.org/pdf/1706.03762.pdf) paper.\n",
        "\n",
        "This notebook dervies heavily from:\n",
        "\n",
        "1. [Pytorch Chatbot Tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html) but instead of RNN Seq2Seq model we will use Transformers.\n",
        "2. [Attention is all you Need notebook](https://github.com/akashe/NLP/blob/main/Attention_is_all_you_need.ipynb) we wrote previosuly.\n",
        "\n",
        "\n",
        "The Pytorch tutorial uses MAX_LENGHT of 10 and removes any conversation pairs with <unk> token and achieves a train NLLloss of 2.4606 after 4000 train steps.\n",
        "\n",
        "Objective:\n",
        "To get NLLloss less that 2.4606 in 4000 iterations using Transformers.\n",
        "\n",
        "Things left:\n",
        "\n",
        "1. reducing qa pairs with max_length and unk token\n",
        "1. restricting to 4000 iterations with random batches\n",
        "2. final qa interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiLz47bYU6h0"
      },
      "source": [
        "### Importing libs and setting device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spjd5q4uTety"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import spacy\n",
        "from torchtext.data import Field,Dataset, Example,BucketIterator\n",
        "import time\n",
        "import pickle,os,re\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdGn4VPXjmTN"
      },
      "source": [
        "SEED = 1007\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3U2MaxWVF7j"
      },
      "source": [
        "### Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7WdjQmMVIjT",
        "outputId": "f8acda7c-5c52-4bfe-92e9-2da03893f2fb"
      },
      "source": [
        "!wget -c http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-08 01:08:36--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_UdDmZqVZF-",
        "outputId": "2fc0bded-04a5-4628-ce9b-4a612614efd3"
      },
      "source": [
        "!unzip -n /content/cornell_movie_dialogs_corpus.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/cornell_movie_dialogs_corpus.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phkyxej4Vr5d",
        "outputId": "fbafae77-a3c4-4adf-b8c6-466cae7e3baa"
      },
      "source": [
        "!head -10 /content/cornell\\ movie-dialogs\\ corpus/movie_lines.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
            "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
            "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
            "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
            "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n",
            "L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n",
            "L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n",
            "L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n",
            "L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\n",
            "L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut0VJNNLXVIp",
        "outputId": "7bff91e7-cf3e-464c-f430-63ef55631b42"
      },
      "source": [
        "!head -10 /content/cornell\\ movie-dialogs\\ corpus/movie_conversations.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEX6UdMKV1oy"
      },
      "source": [
        "### Processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeMXR2QrdoVe"
      },
      "source": [
        "In the pytorch tutorial there are seperate function for:\n",
        "\n",
        "1. Removing unicode character.\n",
        "2. Giving spaces between fullstop and question mark. i.e. Akash? -> Akash ?\n",
        "3. removing redundant spaces\n",
        "\n",
        "Luckily spacy takes care of all of the above while tokenizing. We do have to take care of keeping the text in lower case.\n",
        "\n",
        "Also, since we will be using bucketiterator, we need not worry about setting a max_len for the sentences. Add it to the fact that Transformers dont have an upper limit of src_len or trg_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XllAPZhmg45g"
      },
      "source": [
        "### Loading qa-pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c6HjXOzhCky"
      },
      "source": [
        "def loadLines(fileName, fields):\n",
        "    lines = {}\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            values = line.split(\" +++$+++ \")\n",
        "            # Extract fields\n",
        "            lineObj = {}\n",
        "            for i, field in enumerate(fields):\n",
        "                lineObj[field] = values[i]\n",
        "            lines[lineObj['lineID']] = lineObj\n",
        "    return lines\n",
        "\n",
        "\n",
        "# Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt*\n",
        "def loadConversations(fileName, lines, fields):\n",
        "    conversations = []\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            values = line.split(\" +++$+++ \")\n",
        "            # Extract fields\n",
        "            convObj = {}\n",
        "            for i, field in enumerate(fields):\n",
        "                convObj[field] = values[i]\n",
        "            # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n",
        "            utterance_id_pattern = re.compile('L[0-9]+')\n",
        "            lineIds = utterance_id_pattern.findall(convObj[\"utteranceIDs\"])\n",
        "            # Reassemble lines\n",
        "            convObj[\"lines\"] = []\n",
        "            for lineId in lineIds:\n",
        "                convObj[\"lines\"].append(lines[lineId])\n",
        "            conversations.append(convObj)\n",
        "    return conversations\n",
        "\n",
        "\n",
        "# Extracts pairs of sentences from conversations\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations:\n",
        "        # Iterate over all the lines of the conversation\n",
        "        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n",
        "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "            # Filter wrong samples (if one of the lists is empty)\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])\n",
        "    return qa_pairs"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBUrwmSDhLST",
        "outputId": "46674861-2cba-4a53-eee1-fd7c8424b104"
      },
      "source": [
        "corpus = \"/content/cornell movie-dialogs corpus/\"\n",
        "\n",
        "lines = {}\n",
        "conversations = []\n",
        "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
        "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
        "\n",
        "# Load lines and process conversations\n",
        "print(\"\\nProcessing corpus...\")\n",
        "lines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
        "print(\"\\nLoading conversations...\")\n",
        "conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"),\n",
        "                                  lines, MOVIE_CONVERSATIONS_FIELDS)\n",
        "\n",
        "qa_pairs = extractSentencePairs(conversations)\n",
        "print(f'Total number of qa pairs = {len(qa_pairs)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing corpus...\n",
            "\n",
            "Loading conversations...\n",
            "Total number of qa pairs = 221282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPLeyMmohRIC"
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "# Reducing number of examples by setting max_len = 10\n",
        "length_mask = [len(i[0].split(\" \"))<MAX_LENGTH and len(i[1].split(\" \"))<MAX_LENGTH for i in qa_pairs]\n",
        "\n",
        "qa_pairs = [pair for i,pair in enumerate(qa_pairs) if length_mask[i]]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYGRGGelg7zJ"
      },
      "source": [
        "### Setting up preprocessing necesseties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjMnOqGeV4Mz",
        "outputId": "29a42029-5295-4c81-a201-6fd2c539f79f"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (53.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcvRdD2LcNX0"
      },
      "source": [
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVy4DXHhcaUH",
        "outputId": "c3770862-6a59-4673-d406-6ed5cf23c41d"
      },
      "source": [
        "print([i.text.lower() for i in spacy_en.tokenizer('What!!! No?? \\u1F600')])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['what', '!', '!', '!', 'no', '?', '?', 'ὠ0']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84DdufQ0gTQr"
      },
      "source": [
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzdcCDVciGyR"
      },
      "source": [
        "### Setting up Fields, Vocabs, Datasets and Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZQx1GeVgkqa"
      },
      "source": [
        "# Defining a single field to keep the vocab same for encoder and decoder\n",
        "\n",
        "SRC_TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXrjjzstiKqZ"
      },
      "source": [
        "fields = [('src',SRC_TRG),('trg',SRC_TRG)]\n",
        "\n",
        "chat_examples = [Example.fromlist([pair[0],pair[1]],fields) for pair in qa_pairs]\n",
        "chat_dataset = Dataset(chat_examples,fields)\n",
        "\n",
        "train_data, valid_data = chat_dataset.split(split_ratio=[0.85,0.15],random_state = random.seed(SEED))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1_UOahVlm6O",
        "outputId": "30854c69-55e6-42e8-9fb2-08f2ee5515c1"
      },
      "source": [
        "print(vars(train_data[0]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['he', 'tapped', 'that', '.'], 'trg': ['naw', '!']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9KY45U-lDIr"
      },
      "source": [
        "SRC_TRG.build_vocab(train_data, max_size = 7823,min_freq = 3)\n",
        "# TRG.build_vocab(train_data, max_size = 5000,min_freq = 3)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HuYvjfRnhoN",
        "outputId": "2025ca41-6d8f-4a11-f415-c257b9d32327"
      },
      "source": [
        "# There is no need for the this step. Ideally model should be able to handle unk tokens.\n",
        "# Since we are trying to compare results with the pytorch tutorial,\n",
        "# we are removing pairs with unk token in them\n",
        "pruned_Examples = []\n",
        "for i in train_data.examples:\n",
        "  flag_ = True\n",
        "  for j in i.src:\n",
        "    if j not in SRC_TRG.vocab.stoi:\n",
        "      flag_ = False\n",
        "  for j in i.trg:\n",
        "    if j not in SRC_TRG.vocab.stoi:\n",
        "      flag_ = False\n",
        "  if flag_:\n",
        "    pruned_Examples.append(i)\n",
        "  \n",
        "print(f\"Total number of train_examples after pruning pairs with unk tokens {len(pruned_Examples)}\")\n",
        "train_data.examples = pruned_Examples"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of train_examples after pruning pairs with unk tokens 57320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv2EDAw4j3Fy"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_key = lambda x: len(x.src),\n",
        "     device = device)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCBxfN07nJeK"
      },
      "source": [
        "# Dumps dicts\n",
        "with open(\"/content/SRC_TRG_stio\",\"wb\") as f:\n",
        "  pickle.dump(SRC_TRG.vocab.stoi,f)\n",
        "with open(\"/content/SRC_TRG_itos\",\"wb\") as f:\n",
        "  pickle.dump(SRC_TRG.vocab.itos,f)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLKiWwrPkFMO"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCKkJLYFkLDi"
      },
      "source": [
        "class PositionalEncodingComponent(nn.Module):\n",
        "  '''\n",
        "  Class to encode positional information to tokens.\n",
        "  \n",
        "\n",
        "  '''\n",
        "  def __init__(self,hid_dim,device,dropout=0.2,max_len=5000):\n",
        "    super().__init__()\n",
        "\n",
        "    assert hid_dim%2==0 # If not, it will result error in allocation to positional_encodings[:,1::2] later\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.positional_encodings = torch.zeros(max_len,hid_dim)\n",
        "\n",
        "    pos = torch.arange(0,max_len).unsqueeze(1) # pos : [max_len,1]\n",
        "    div_term  = torch.exp(-torch.arange(0,hid_dim,2)*math.log(10000.0)/hid_dim) # Calculating value of 1/(10000^(2i/hid_dim)) in log space and then exponentiating it\n",
        "    # div_term: [hid_dim//2]\n",
        "\n",
        "    self.positional_encodings[:,0::2] = torch.sin(pos*div_term) # pos*div_term [max_len,hid_dim//2]\n",
        "    self.positional_encodings[:,1::2] = torch.cos(pos*div_term) \n",
        "\n",
        "    self.positional_encodings = self.positional_encodings.unsqueeze(0) # To account for batch_size in inputs\n",
        "    # positional_encodings : [1,max_len,hid_dim]\n",
        "    \n",
        "    self.device = device\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.positional_encodings[:,:x.size(1)].detach().to(self.device)\n",
        "    return self.dropout(x)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPxIbk8QkaGo"
      },
      "source": [
        "class FeedForwardComponent(nn.Module):\n",
        "  '''\n",
        "  Class for pointwise feed forward connections\n",
        "  '''\n",
        "  def __init__(self,hid_dim,pf_dim,dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.fc1 = nn.Linear(hid_dim,pf_dim)\n",
        "    self.fc2 = nn.Linear(pf_dim,hid_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    # x : [batch_size,seq_len,hid_dim]\n",
        "    x = self.dropout(torch.relu(self.fc1(x)))\n",
        "\n",
        "    # x : [batch_size,seq_len,pf_dim]\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    # x : [batch_size,seq_len,hid_dim]\n",
        "    return x"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFLRDzXxkbzC"
      },
      "source": [
        "class MultiHeadedAttentionComponent(nn.Module):\n",
        "  '''\n",
        "  Multiheaded attention Component. This implementation also supports mask. \n",
        "  The reason for mask that in Decoder, we don't want attention mechanism to get\n",
        "  important information from future tokens.\n",
        "  '''\n",
        "  def __init__(self,hid_dim, n_heads, dropout, device):\n",
        "    super().__init__()\n",
        "\n",
        "    assert hid_dim % n_heads == 0 # Since we split hid_dims into n_heads\n",
        "\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_heads = n_heads # no of heads in 'multiheaded' attention\n",
        "    self.head_dim = hid_dim//n_heads # dims of each head\n",
        "\n",
        "    # Transformation from source vector to query vector\n",
        "    self.fc_q = nn.Linear(hid_dim,hid_dim)\n",
        "\n",
        "    # Transformation from source vector to key vector\n",
        "    self.fc_k = nn.Linear(hid_dim,hid_dim)\n",
        "\n",
        "    # Transformation from source vector to value vector\n",
        "    self.fc_v = nn.Linear(hid_dim,hid_dim)\n",
        "\n",
        "    self.fc_o = nn.Linear(hid_dim,hid_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Used in self attention for smoother gradients\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "  def forward(self,query,key,value,mask=None):\n",
        "\n",
        "    #query : [batch_size, query_len, hid_dim]\n",
        "    #key : [batch_size, key_len, hid_dim]\n",
        "    #value : [batch_size, value_len, hid_dim]\n",
        "\n",
        "    batch_size = query.shape[0]\n",
        "\n",
        "    # Transforming quey,key,values\n",
        "    Q = self.fc_q(query)\n",
        "    K = self.fc_k(key)\n",
        "    V = self.fc_v(value)\n",
        "\n",
        "    #Q : [batch_size, query_len, hid_dim]\n",
        "    #K : [batch_size, key_len, hid_dim]\n",
        "    #V : [batch_size, value_len,hid_dim]\n",
        "\n",
        "    # Changing shapes to acocmadate n_heads information\n",
        "    Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "    #Q : [batch_size, n_heads, query_len, head_dim]\n",
        "    #K : [batch_size, n_heads, key_len, head_dim]\n",
        "    #V : [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "    # Calculating alpha\n",
        "    score = torch.matmul(Q,K.permute(0,1,3,2))/self.scale\n",
        "    # score : [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "    if mask is not None:\n",
        "      score = score.masked_fill(mask==0,-1e10)\n",
        "\n",
        "    alpha = torch.softmax(score,dim=-1)\n",
        "    # alpha : [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "    # Get the final self-attention  vector\n",
        "    x = torch.matmul(self.dropout(alpha),V)\n",
        "    # x : [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "    # Reshaping self attention vector to concatenate\n",
        "    x = x.permute(0,2,1,3).contiguous()\n",
        "    # x : [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "    x = x.view(batch_size,-1,self.hid_dim)\n",
        "    # x: [batch_size, query_len, hid_dim]\n",
        "\n",
        "    # Transforming concatenated outputs \n",
        "    x = self.fc_o(x)\n",
        "    #x : [batch_size, query_len, hid_dim] \n",
        "\n",
        "    return x, alpha"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwAaA_SXkcWr"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  '''\n",
        "  Operations of a single layer in an Encoder. An Encoder employs multiple such layers. Each layer contains:\n",
        "  1) multihead attention, folllowed by\n",
        "  2) LayerNorm of addition of multihead attention output and input to the layer, followed by\n",
        "  3) FeedForward connections, followed by\n",
        "  4) LayerNorm of addition of FeedForward outputs and output of previous layerNorm.\n",
        "  '''\n",
        "  def __init__(self, hid_dim,n_heads,pf_dim,dropout,device):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.self_attn_layer_norm = nn. LayerNorm(hid_dim) #Layer norm after self-attention\n",
        "    self.ff_layer_norm = nn.LayerNorm(hid_dim) # Layer norm after FeedForward component\n",
        "\n",
        "    self.self_attention = MultiHeadedAttentionComponent(hid_dim,n_heads,dropout,device)\n",
        "    self.feed_forward = FeedForwardComponent(hid_dim,pf_dim,dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self,src,src_mask):\n",
        "    \n",
        "    # src : [batch_size, src_len, hid_dim]\n",
        "    # src_mask : [batch_size, 1, 1, src_len]\n",
        "\n",
        "    # get self-attention\n",
        "    _src, _ = self.self_attention(src,src,src,src_mask)\n",
        "\n",
        "    # LayerNorm after dropout\n",
        "    src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "    # src : [batch_size, src_len, hid_dim]\n",
        "\n",
        "    # FeedForward\n",
        "    _src = self.feed_forward(src)\n",
        "\n",
        "    # layerNorm after dropout\n",
        "    src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "    # src: [batch_size, src_len, hid_dim]\n",
        "\n",
        "    return src\n",
        "    "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWqnW-tWkdHb"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  '''\n",
        "  Operations of a single layer in an Decoder. An Decoder employs multiple such layers. Each layer contains:\n",
        "  1) masked decoder self attention, followed by\n",
        "  2) LayerNorm of addition of previous attention output and input to the layer,, followed by\n",
        "  3) encoder self attention, followed by\n",
        "  4) LayerNorm of addition of result of encoder self attention and its input, followed by\n",
        "  5) FeedForward connections, followed by\n",
        "  6) LayerNorm of addition of Feedforward results and its input.\n",
        "  '''\n",
        "  def __init__(self,hid_dim,n_heads,pf_dim,dropout,device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "\n",
        "    # decoder self attention\n",
        "    self.self_attention = MultiHeadedAttentionComponent(hid_dim,n_heads,dropout,device)\n",
        "\n",
        "    # encoder attention\n",
        "    self.encoder_attention = MultiHeadedAttentionComponent(hid_dim,n_heads,dropout,device)\n",
        "\n",
        "    # FeedForward\n",
        "    self.feed_forward = FeedForwardComponent(hid_dim,pf_dim,dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,trg, enc_src,trg_mask,src_mask):\n",
        "\n",
        "    #trg : [batch_size, trg_len, hid_dim]\n",
        "    #enc_src : [batch_size, src_len, hid_dim]\n",
        "    #trg_mask : [batch_size, 1, trg_len, trg_len]\n",
        "    #src_mask : [batch_size, 1, 1, src_len]\n",
        "\n",
        "    '''\n",
        "    Decoder self-attention\n",
        "    trg_mask is to force decoder to look only into past tokens and not get information from future tokens.\n",
        "    Since we apply mask before doing softmax, the final self attention vector gets no information from future tokens.\n",
        "    '''\n",
        "    _trg, _ = self.self_attention(trg,trg,trg,trg_mask)\n",
        "\n",
        "    # LayerNorm and dropout with resdiual connection\n",
        "    trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "    # trg : [batch_size, trg_len, hid_dim]\n",
        "\n",
        "    '''\n",
        "    Encoder attention:\n",
        "    Query: trg\n",
        "    key: enc_src\n",
        "    Value : enc_src\n",
        "    Why? \n",
        "    the idea here is to extract information from encoder outputs. So we use decoder self-attention as a query to find important values from enc_src\n",
        "    and that is why we use src_mask, to avoid getting information from enc_src positions where it is equal to pad-id\n",
        "    After we get necessary infromation from encoder outputs we add them back to decoder self-attention.\n",
        "    '''\n",
        "    _trg, encoder_attn_alpha = self.encoder_attention(trg,enc_src,enc_src,src_mask)\n",
        "\n",
        "    # LayerNorm , residual connection and dropout\n",
        "    trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "    # trg : [ batch_size, trg_len, hid_dim]\n",
        "\n",
        "    # Feed Forward\n",
        "    _trg = self.feed_forward(trg)\n",
        "\n",
        "    # LayerNorm, residual connection and dropout\n",
        "    trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    return trg, encoder_attn_alpha\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byffMNI8kfKe"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  '''\n",
        "  An encoder, creates token embeddings and position embeddings and passes them through multiple encoder layers\n",
        "  This is a bidirectional encoder. So we will reverse the input, add appropriate padding and positional embeddings and\n",
        "  concat it with the forward input.\n",
        "  '''\n",
        "  def __init__(self,input_dim,hid_dim,n_layers,n_heads,pf_dim,dropout,device,max_length = 5000):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.tok_embedding = nn.Embedding(input_dim,hid_dim)\n",
        "    self.pos_embedding = PositionalEncodingComponent(hid_dim,device,dropout,max_length)\n",
        "\n",
        "    # encoder layers\n",
        "    self.layers = nn.ModuleList([EncoderLayer(2*hid_dim,n_heads,pf_dim,dropout,device) for _ in range(n_layers)])\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "    self.hid_dim = hid_dim\n",
        "\n",
        "  def reverse_src(self,src,src_mask):\n",
        "    # src : [batch_size, src_len]\n",
        "    # src_mask : [batch_size,1,1,src_len]\n",
        "\n",
        "    total_non_padded_values = src_mask.sum(dim=-1).squeeze(1)\n",
        "    # total_non_padded_values : [batch_size,1]\n",
        "\n",
        "    reversed_src = torch.ones(src.shape,dtype=torch.int64)\n",
        "    # reversed_src : [batch_size,src_len]\n",
        "\n",
        "    # flip the src :[10,9,8] -> [8,9,10]\n",
        "    src = torch.flip(src,dims=[-1])\n",
        "    # src : [batch_size, src_len]\n",
        "    \n",
        "    for pos,(i,j) in enumerate(zip(src,total_non_padded_values)):\n",
        "      j = int(j.item())\n",
        "      reversed_src[pos] = torch.cat([i[-j:],i[:-j]],dim=-1)\n",
        "\n",
        "    return reversed_src\n",
        "  \n",
        "  def forward(self,src,src_mask):\n",
        "\n",
        "    # src : [batch_size, src_len]\n",
        "    # src_mask : [batch_size,1,1,src_len]\n",
        "\n",
        "    batch_size = src.shape[0]\n",
        "    src_len = src.shape[1]\n",
        "\n",
        "    # reversed source\n",
        "    reversed_src = self.reverse_src(src,src_mask).to(device)\n",
        "    # reversed_src : [batch_size,src_len]\n",
        "\n",
        "    tok_embeddings = self.tok_embedding(src)*self.scale\n",
        "    reversed_tok_embeddings = self.tok_embedding(reversed_src)*self.scale\n",
        "\n",
        "    # token plus position embeddings\n",
        "    src  = self.pos_embedding(tok_embeddings)\n",
        "    reversed_src = self.pos_embedding(reversed_tok_embeddings)\n",
        "\n",
        "    concatenated_src = torch.ones([batch_size,src_len,self.hid_dim*2]).to(device)\n",
        "    # Interleaving concatenated source such that f1,b1,f2,b2....\n",
        "    concatenated_src[:,:,0::2] = src\n",
        "    concatenated_src[:,:,1::2] = reversed_src\n",
        "\n",
        "    src = concatenated_src\n",
        "\n",
        "    for layer in self.layers:\n",
        "      src = layer(src,src_mask)\n",
        "    # src : [batch_size, src_len, 2*hid_dim]\n",
        "\n",
        "    return src"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8fn9uHrkhKF"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  '''\n",
        "  An decoder, creates token embeddings and position embeddings and passes them through multiple decoder layers\n",
        "  '''\n",
        "  def __init__(self,output_dim,hid_dim,n_layers,n_heads,pf_dim,dropout,device,max_length= 5000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.tok_embedding = nn.Embedding(output_dim,hid_dim)\n",
        "    self.pos_embedding = PositionalEncodingComponent(hid_dim,device,dropout,max_length)\n",
        "\n",
        "    # decoder layers\n",
        "    self.layers = nn.ModuleList([DecoderLayer(hid_dim,n_heads,pf_dim,dropout,device) for _ in range(n_layers)])\n",
        "\n",
        "    # convert decoder outputs to real outputs\n",
        "    self.fc_out = nn.Linear(hid_dim,output_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "  def forward(self, trg, enc_src,trg_mask,src_mask,mode='train'):\n",
        "    \n",
        "    #trg : [batch_size, trg_len]\n",
        "    #enc_src : [batch_size, src_len, hid_dim]\n",
        "    #trg_mask : [batch_size, 1, trg_len, trg_len]\n",
        "    #src_mask : [batch_size, 1, 1, src_len]\n",
        "\n",
        "    batch_size = trg.shape[0]\n",
        "    trg_len = trg.shape[1]\n",
        "\n",
        "    tok_embeddings = self.tok_embedding(trg)*self.scale\n",
        "\n",
        "    # token plus pos embeddings\n",
        "    if not mode=='infer':\n",
        "      trg = self.pos_embedding(tok_embeddings)\n",
        "    else:\n",
        "      trg = tok_embeddings\n",
        "    # trg : [batch_size, trg_len, hid_dim]\n",
        "\n",
        "    # Pass trg thorugh decoder layers\n",
        "    for layer in self.layers:\n",
        "      trg, encoder_attention = layer(trg,enc_src,trg_mask,src_mask)\n",
        "    \n",
        "    # trg : [batch_size,trg_len,hid_dim]\n",
        "    # encoder_attention :  [batch_size, n_head,trg_len, src_len]\n",
        "\n",
        "    # Convert to outputs\n",
        "    output = self.fc_out(trg)\n",
        "    # output : [batch_size, trg_len, output_dim]\n",
        "    \n",
        "    return output, encoder_attention"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dBBnH83kiSI"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def make_src_mask(self,src):\n",
        "    # src : [batch_size, src_len]\n",
        "\n",
        "    # Masking pad values\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    # src_mask : [batch_size,1,1,src_len]\n",
        "\n",
        "    return src_mask\n",
        "\n",
        "  def make_trg_mask(self,trg):\n",
        "    # trg : [batch_size, trg_len]\n",
        "\n",
        "    # Masking pad values\n",
        "    trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    # trg_pad_mask : [batch_size,1,1, trg_len]\n",
        "\n",
        "    # Masking future values\n",
        "    trg_len = trg.shape[1]\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len,trg_len),device= self.device)).bool()\n",
        "    # trg_sub_mask : [trg_len, trg_len]\n",
        "\n",
        "    # combine both masks\n",
        "    trg_mask = trg_pad_mask & trg_sub_mask\n",
        "    # trg_mask = [batch_size,1,trg_len,trg_len]\n",
        "\n",
        "    return trg_mask\n",
        "\n",
        "  def forward(self,src,trg):\n",
        "\n",
        "    # src : [batch_size, src_len]\n",
        "    # trg : [batch_size, trg_len]\n",
        "\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "    # src_mask : [ batch_size, 1,1,src_len]\n",
        "    # trg_mask : [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "    enc_src = self.encoder(src,src_mask)\n",
        "    #enc_src : [batch_size, src_len, hid_dim]\n",
        "\n",
        "    output, encoder_decoder_attention = self.decoder(trg,enc_src,trg_mask,src_mask)\n",
        "    # output : [batch_size, trg_len, output_dim]\n",
        "    # encoder_decoder_attention : [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "    return output, encoder_decoder_attention"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_YLil4tkwAD"
      },
      "source": [
        "### Initializing Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O9NyH8EkzlI"
      },
      "source": [
        "INPUT_DIM = len(SRC_TRG.vocab)\n",
        "OUTPUT_DIM = len(SRC_TRG.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 1\n",
        "DEC_LAYERS = 1\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              2*HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "SRC_PAD_IDX = SRC_TRG.vocab.stoi[SRC_TRG.pad_token]\n",
        "TRG_PAD_IDX = SRC_TRG.vocab.stoi[SRC_TRG.pad_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g4fXGjplapO"
      },
      "source": [
        "### Initialize weights and total model params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7_9p1j3lhdi"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights);"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba45ySktlito",
        "outputId": "ee2ab07e-e827-4a26-e726-0a40971e492f"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 14,234,003 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj5NlAsml7if"
      },
      "source": [
        "### Learning rate, criterion and optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk5WtaZWmABj",
        "outputId": "fecdc131-41d6-41d0-9d56-b2924203b733"
      },
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "decoder_learning_ratio = 5\n",
        "\n",
        "encoder_optimizer = torch.optim.Adam(model.encoder.parameters(), lr = LEARNING_RATE)\n",
        "decoder_optimizer = torch.optim.Adam(model.decoder.parameters(),lr= decoder_learning_ratio*LEARNING_RATE)\n",
        "\n",
        "optimizer = (encoder_optimizer,decoder_optimizer)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX,size_average= True) # size average = True calculates mean loss only of non pad idx values"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLyoB2vrmB3j"
      },
      "source": [
        "### Train and Eval Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzY5PQalmEud"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip,iters):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer[0].zero_grad()\n",
        "        optimizer[1].zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        if iters == 4000:\n",
        "          print(f'Train loss at {iters} iteration is {loss.item()} ')\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer[0].step()\n",
        "        optimizer[1].step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        iters += 1\n",
        "\n",
        "    return epoch_loss / len(iterator), iters"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnVaJt5AmGGt"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z9AObu6mJyn"
      },
      "source": [
        "### Runner Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTWRwHhgmMco"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiffzngEmNwX",
        "outputId": "ad491dfe-5ef2-46c6-d58b-d5b6357c105a"
      },
      "source": [
        "N_EPOCHS = 15 \n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "iters = 0\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss,iters = train(model, train_iterator, optimizer, criterion, CLIP ,iters)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'chat_bot_model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 32s\n",
            "\tTrain Loss: 3.945 | Train PPL:  51.692\n",
            "\t Val. Loss: 3.984 |  Val. PPL:  53.746\n",
            "Epoch: 02 | Time: 0m 31s\n",
            "\tTrain Loss: 3.506 | Train PPL:  33.312\n",
            "\t Val. Loss: 3.926 |  Val. PPL:  50.709\n",
            "Epoch: 03 | Time: 0m 31s\n",
            "\tTrain Loss: 3.339 | Train PPL:  28.180\n",
            "\t Val. Loss: 3.931 |  Val. PPL:  50.936\n",
            "Epoch: 04 | Time: 0m 31s\n",
            "\tTrain Loss: 3.210 | Train PPL:  24.775\n",
            "\t Val. Loss: 3.970 |  Val. PPL:  52.974\n",
            "Train loss at 4000 iteration is 3.011871814727783 \n",
            "Epoch: 05 | Time: 0m 32s\n",
            "\tTrain Loss: 3.101 | Train PPL:  22.212\n",
            "\t Val. Loss: 3.995 |  Val. PPL:  54.309\n",
            "Epoch: 06 | Time: 0m 32s\n",
            "\tTrain Loss: 3.004 | Train PPL:  20.175\n",
            "\t Val. Loss: 4.033 |  Val. PPL:  56.428\n",
            "Epoch: 07 | Time: 0m 31s\n",
            "\tTrain Loss: 2.916 | Train PPL:  18.476\n",
            "\t Val. Loss: 4.109 |  Val. PPL:  60.889\n",
            "Epoch: 08 | Time: 0m 31s\n",
            "\tTrain Loss: 2.836 | Train PPL:  17.042\n",
            "\t Val. Loss: 4.143 |  Val. PPL:  63.004\n",
            "Epoch: 09 | Time: 0m 31s\n",
            "\tTrain Loss: 2.759 | Train PPL:  15.782\n",
            "\t Val. Loss: 4.226 |  Val. PPL:  68.426\n",
            "Epoch: 10 | Time: 0m 31s\n",
            "\tTrain Loss: 2.690 | Train PPL:  14.734\n",
            "\t Val. Loss: 4.285 |  Val. PPL:  72.607\n",
            "Epoch: 11 | Time: 0m 32s\n",
            "\tTrain Loss: 2.628 | Train PPL:  13.851\n",
            "\t Val. Loss: 4.341 |  Val. PPL:  76.762\n",
            "Epoch: 12 | Time: 0m 31s\n",
            "\tTrain Loss: 2.570 | Train PPL:  13.067\n",
            "\t Val. Loss: 4.402 |  Val. PPL:  81.652\n",
            "Epoch: 13 | Time: 0m 32s\n",
            "\tTrain Loss: 2.517 | Train PPL:  12.388\n",
            "\t Val. Loss: 4.462 |  Val. PPL:  86.698\n",
            "Epoch: 14 | Time: 0m 32s\n",
            "\tTrain Loss: 2.463 | Train PPL:  11.736\n",
            "\t Val. Loss: 4.550 |  Val. PPL:  94.641\n",
            "Epoch: 15 | Time: 0m 31s\n",
            "\tTrain Loss: 2.422 | Train PPL:  11.266\n",
            "\t Val. Loss: 4.569 |  Val. PPL:  96.411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFaQVn0mjeAS"
      },
      "source": [
        "### Chatbot Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkhYCILGo0Yx"
      },
      "source": [
        "#### Load dicts and model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqZtA3T_nodw",
        "outputId": "8b8ea66d-5089-4728-d22b-30eb10e28b4f"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"SRC_TRG_stio\",\"rb\") as f:\n",
        "  stoi = pickle.load(f)\n",
        "with open(\"SRC_TRG_itos\",\"rb\") as f:\n",
        "  itos = pickle.load(f)\n",
        "\n",
        "# Load model\n",
        "trained_model = 'chat_bot_model.pt'\n",
        "model.load_state_dict(torch.load(trained_model));\n",
        "model.eval()\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(7827, 256)\n",
              "    (pos_embedding): PositionalEncodingComponent(\n",
              "      (dropout): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (feed_forward): FeedForwardComponent(\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(7827, 512)\n",
              "    (pos_embedding): PositionalEncodingComponent(\n",
              "      (dropout): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadedAttentionComponent(\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (feed_forward): FeedForwardComponent(\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=512, out_features=7827, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXf07tXpo3ND"
      },
      "source": [
        "#### Encode inputs and decode outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah57yqRRpzbS"
      },
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def encode_inputs(input,vocab):\n",
        "\n",
        "  tokenized_input = [tok.text.lower() for tok in spacy_en.tokenizer(input)]\n",
        "  tokenized_input = ['<sos>'] + tokenized_input +['<eos>']\n",
        "\n",
        "  numericalized_input = [vocab[i] for i in tokenized_input]\n",
        "\n",
        "  tensor_input = torch.LongTensor([numericalized_input])\n",
        "  \n",
        "  return tensor_input\n",
        "\n",
        "def decode_outputs(output,vocab):\n",
        "  # output: [1,1,hid_dim]\n",
        "  predicted_token = output.argmax(-1)\n",
        "  return vocab[predicted_token.item()], predicted_token\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHcR5hfquBgJ"
      },
      "source": [
        "#### Interaction loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqQeI701uEjx",
        "outputId": "9cf3340e-b753-48f9-d47e-63ca40eb6ec7"
      },
      "source": [
        "print(\" Enter q or quit to exit.\")\n",
        "\n",
        "chatbot_answer_max_len = 10\n",
        "\n",
        "while(True):\n",
        "\n",
        "  input_ = input(\"Enter text:\")\n",
        "\n",
        "  if input_=='q' or input_=='quit':\n",
        "    break\n",
        "\n",
        "  src = encode_inputs(input_,stoi).to(device)\n",
        "  src_mask = torch.ones([1,1,1,src.shape[-1]]).to(device)\n",
        "\n",
        "  trg = '<sos>'\n",
        "  trg = torch.LongTensor([stoi[trg]]).unsqueeze(0).to(device)\n",
        "  trg_mask = torch.ones([1,1,1,1]).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    enc_src = model.encoder(src,src_mask)\n",
        "  \n",
        "  decoder_outputs = []\n",
        "  for i in range(chatbot_answer_max_len):\n",
        "\n",
        "    with torch.no_grad():\n",
        "      decoder_output,_ = model.decoder(trg,enc_src,trg_mask,src_mask,mode='infer')\n",
        "\n",
        "    decoder_output_itos_value, trg = decode_outputs(decoder_output,itos)\n",
        "\n",
        "    if decoder_output_itos_value == '<eos>':\n",
        "      break\n",
        "    decoder_outputs.append(decoder_output_itos_value)\n",
        "\n",
        "    \n",
        "  print(\" \".join(decoder_outputs))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Enter q or quit to exit.\n",
            "Enter text:hi\n",
            "you 're not good .\n",
            "Enter text:what\n",
            "i 'll be right .\n",
            "Enter text:oh boy you didn't train well\n",
            "i 'm not really ?\n",
            "Enter text:yes\n",
            "i 'll be right .\n",
            "Enter text:you are overfitted\n",
            "i 'm not really ?\n",
            "Enter text:yes\n",
            "i 'll be right .\n",
            "Enter text:q\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}