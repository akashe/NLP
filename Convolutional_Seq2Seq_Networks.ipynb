{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Seq2Seq Networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM5fKFC+9aNWpWCTvnMEdM/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashe/NLP/blob/main/Convolutional_Seq2Seq_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJDtboBPEQ5G"
      },
      "source": [
        "In this notebook, we discuss the steps in a Convolutional sequence and sequence to networks in detail. The explanations are mine but the code is not mine. Its copied from other places.\n",
        "\n",
        "paper: https://arxiv.org/pdf/1612.08083.pdf\n",
        "\n",
        "\n",
        "Architecture:\n",
        "![IMg](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY0vR8_GGkSF"
      },
      "source": [
        "\n",
        "In a convolution seq2seq model, the most important deviation from RNN based sequence to sequence model is that the input is sent in one go. There is no unrolling in sequence dimension.\n",
        "\n",
        "Important concepts:\n",
        "\n",
        "1. Positional Embeddings: Since there is no concept of unrolling, we provide sequence information in the form of postional embeddings. We add these positonal embeddings with the input dims.\n",
        "2. Residual connections:\n",
        "  *  help pass on the input information to each part of the architecture. So each layer has the option to look at previous layers ouputs/features in conjuction with the original input.\n",
        "  * help in gradient flow. Each layer of the architecture gets a good enough magnitude of gradients. In networks with no residual connections, the gradients keep diminishing with each layer resulting in difficult training.\n",
        "3. GLU activation: Conv Seq2Seq employ GLU activation. Given a vector $v$, it divides the vector into two parts, $(a,b)$ along a given dimension. It then performs: \n",
        "  * $a\\otimes\\sigma(b)$.\n",
        "  * This can be interpreted as choosing features from $b$ using sigmoid gate(which outputs from 0 to 1) and amplifying the corresponding features in $a$. It is the same kind of operation we do in a LSTM in forget gate.\n",
        "4. Kernels: **The name is convolution so it will have kernels!** The inputs in this model are of the form [batch_size,hid_dim,src_len]. Here, 'hid_dim' acts as input channels. And we perform convolution in 'src_len' dim. Since we are convolving along 1 direction, we use Conv1d.\n",
        "If the sentence is \"I am the king of the world\". The next table shows how we convolve for kernel size 3:\n",
        "\n",
        "|sentence|I|am|the|king|of|the|world|\n",
        "| :------: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
        "| Kernel position 1| i | am | the|\n",
        "| Kernel position 2| | am |the| king|\n",
        "| Kernel position 3| | | the | king| of|\n",
        "| Kernel position 4| | | | king | of | the|\n",
        "| Kernel position 5| | | | | of | the| world\n",
        "\n",
        "\n",
        "5. Padding: In the above example, the source length of the sentence,\"I am the king of the world\" was 7. We had 5 kernel positions. So the output length will be 5. When we have multiple layers of convolution this could be problematic to write code. So, we use padding to keep the input and output source lenghts same. \n",
        "  * $\\mathcal{Padding}=Kernel Size -1$\n",
        "  * padding happens on both sides of the input. So our source sentence becomes: **\\<pad\\> I am the king of the world \\<pad\\>**\n",
        "  * In decoders, as we will see, we add the padding in the beggining of the sentence. *We do so because we want to avoid decoder from looking into future information and keeping the output len same as source len*. For example for Kernel size 3 padding in decoder will look like this:\n",
        "\n",
        "|sentence|\\<pad\\>|\\<pad\\>|I|am|the|king|of|the|world|\n",
        "| :------: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |:---: |:---: |\n",
        "| Kernel for pos 1 |\\<pad\\>|\\<pad\\>|I|\n",
        "| Kernel for pos 2 | |\\<pad\\>|I|am|\n",
        "| Kernel for pos 3 | | |I|am|the|\n",
        "| Kernel for pos 4 | | | |am|the|king|\n",
        "| Kernel for pos 5 | | | | |the|king|of|\n",
        "| Kernel for pos 6 | | | | | | king|of|the|\n",
        "| Kernel for pos 7 | | | | | | |of|the|world|\n",
        "\n",
        "\n",
        "6. Attention: How do we use the information from encoder in decoder? In RNN, we used the last hidden vector as an input to decoder RNN. *Attention is RNN models works as an auxillary functionality* to extract relevant information from encoder hidden states. *Since Convolution Seq2Seq don't have rolling in sequence dimension, they use Attention as the main function to extract information from encoder.*\n",
        "  * Encoder outputs two vectors:\n",
        "    1. $\\text{encoder_conved}$: Output of convolution operations. Note: encoder employs multiple convolution layers for maximum feature extraction. 'encoder_conved' is the output of the last layer.\n",
        "    2. $\\text{encoder_combined}$ As discussed in Residual connections, 'encoder_combined' is the sum of the 'encoder_conved' with 'input_representations'. This way encoder outputs retain some information of the inputs.\n",
        "  * In decoder, after feature extraction with convolutions, we use recently extracted features to find what elements of $\\text{encoder_conved}$ are most important/aligned to it. We use this importance as a ratio of how much information to take from $\\text{encoder_combined}$. This mechanism of finding relavant information and selectively using the relavant information is called Attention.\n",
        "  * Notice that unlike RNN models, here we calculate the attention at the same for the entire output sequence at once! and we can repeat finding attention for each decoder convolutional layer!!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8j-YreBD2dx"
      },
      "source": [
        "Importing libs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqf6kpBvFFcT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2yBI_LXFDBE"
      },
      "source": [
        "Setting Seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilTCAFbgFJWw"
      },
      "source": [
        "SEED = 7777\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37icglTkFKDN"
      },
      "source": [
        "Downloading spacy models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpmh3zBLFOh_",
        "outputId": "116327cf-faf2-4dc7-ad56-ff54ee80bb52"
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro70ysNUFPSe"
      },
      "source": [
        "Loading spacy models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_q9WL1RFS9q"
      },
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICRX5P5qFhRs"
      },
      "source": [
        "Defining tokenizer functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lKF5gYwFYUo"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLM-hMw4FkVz"
      },
      "source": [
        "Define Fields for processing:\n",
        "Note: since we are working with convolutions, we set batch_first= True to get dims like [batch_size,src_len,input_dim]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE4eUWuBF17u"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4iFJbZzF37_"
      },
      "source": [
        "Downloading Multi30k german to english translation dataset and setting train,valid and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc6TX-jUGLw6"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), \n",
        "                                                    fields=(SRC, TRG))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-7LCaR7GNkz"
      },
      "source": [
        "Building vocabs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is0re_GqGQK2"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk6-FYQbGQ-s"
      },
      "source": [
        "Setting device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVxAGKaAGSuZ"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSyRBqB7GUQ2"
      },
      "source": [
        "Using bucketIterator to get batches with similar or closer source lengths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uLisozHGgWO"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     device = device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqn4Rg0yZN8-"
      },
      "source": [
        "Encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_gQJ3asZPCh"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 emb_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 kernel_size, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        # scale factor, we use to scale down sum of 2 tensors\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        # Convert input from input_dims to emb dims\n",
        "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        # Convert positional information to emb_dim\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
        "        \n",
        "        # Convert tensors to hid_dim to feed them to Convolutional blocks\n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        # Convert tensors to convert them to emb_dim after convolutional blocks to be used by decoder\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        \n",
        "        # layers of 1D convolutions which put padding in start and end of sentence\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
        "                                              out_channels = 2 * hid_dim, \n",
        "                                              kernel_size = kernel_size, \n",
        "                                              padding = (kernel_size - 1) // 2)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        # Dropout to improve performance by regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [0, 1, 2, 3, ..., src len - 1]\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(src)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\n",
        "        \n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #embedded = [batch size, src len, emb dim]\n",
        "        \n",
        "        #pass embedded through linear layer to convert from emb dim to hid dim\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "        \n",
        "        #conv_input = [batch size, src len, hid dim]\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) \n",
        "        \n",
        "        #conv_input = [batch size, hid dim, src len]\n",
        "        \n",
        "        #begin convolutional blocks...\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(self.dropout(conv_input))\n",
        "\n",
        "            #conved = [batch size, 2 * hid dim, src len]\n",
        "\n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim = 1)\n",
        "\n",
        "            #conved = [batch size, hid dim, src len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "\n",
        "            #conved = [batch size, hid dim, src len]\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration\n",
        "            conv_input = conved\n",
        "        \n",
        "        #...end convolutional blocks\n",
        "        \n",
        "        #permute and convert back to emb dim\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        #conved = [batch size, src len, emb dim]\n",
        "        \n",
        "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
        "        combined = (conved + embedded) * self.scale\n",
        "        \n",
        "        #combined = [batch size, src len, emb dim]\n",
        "        \n",
        "        return conved, combined"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJU37Nw8ZP-Z"
      },
      "source": [
        "Decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdKOxu95ZR6o"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 emb_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 kernel_size, \n",
        "                 dropout, \n",
        "                 trg_pad_idx, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        # setting kernel size\n",
        "        self.kernel_size = kernel_size\n",
        "        # setting target padding index\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device # device info\n",
        "        \n",
        "        # scale tensor to scale down sum of two vectors\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        # Embedding to convert target sentence from output_dim to emb_dim\n",
        "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        # Embedding to convert position embeddings of the target sentence to emb_dim\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
        "        \n",
        "        # Converting tensor to hid_dim to be used by Convolution layers\n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        # Converting tensor to emb_dim to convert them target disctionaries\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        \n",
        "        # convert convoution outputs to emb_dim to work with encoder outputs which are in emb_dim\n",
        "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        # convert the outputs of attention mechanism to hid_dim to be used by next convolutional layer\n",
        "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        \n",
        "        # Convert to the target dictionary\n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
        "        \n",
        "        # layers of 1D convolutions\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
        "                                              out_channels = 2 * hid_dim, \n",
        "                                              kernel_size = kernel_size)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
        "        '''\n",
        "        Calculate attention bw conved + embedded bw encoder_conved and use the alignment to extarct information from encoder_combined \n",
        "        which is encoder_conved+encoder_inputs\n",
        "        '''\n",
        "        \n",
        "        #embedded = [batch size, trg len, emb dim]\n",
        "        #conved = [batch size, hid dim, trg len]\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
        "        \n",
        "        #permute and convert back to emb dim\n",
        "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        #conved_emb = [batch size, trg len, emb dim]\n",
        "        \n",
        "        combined = (conved_emb + embedded) * self.scale\n",
        "        \n",
        "        #combined = [batch size, trg len, emb dim]\n",
        "        # Find alpha values bw combined and encoder_conved        \n",
        "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
        "        \n",
        "        #energy = [batch size, trg len, src len]\n",
        "        \n",
        "        # performa softmax so you can get scaled importance\n",
        "        attention = F.softmax(energy, dim=2)\n",
        "        \n",
        "        #attention = [batch size, trg len, src len]\n",
        "\n",
        "        # get the scaled values from encoder_combined    \n",
        "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
        "        \n",
        "        #attended_encoding = [batch size, trg len, emd dim]\n",
        "        \n",
        "        #convert from emb dim -> hid dim\n",
        "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
        "        \n",
        "        #attended_encoding = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #apply residual connection\n",
        "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
        "        \n",
        "        #attended_combined = [batch size, hid dim, trg len]\n",
        "        \n",
        "        return attention, attended_combined\n",
        "        \n",
        "    def forward(self, trg, encoder_conved, encoder_combined):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "            \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, trg len]\n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(trg)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        #tok_embedded = [batch size, trg len, emb dim]\n",
        "        #pos_embedded = [batch size, trg len, emb dim]\n",
        "        \n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #embedded = [batch size, trg len, emb dim]\n",
        "        \n",
        "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "        \n",
        "        #conv_input = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) \n",
        "        \n",
        "        #conv_input = [batch size, hid dim, trg len]\n",
        "        \n",
        "        batch_size = conv_input.shape[0]\n",
        "        hid_dim = conv_input.shape[1]\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #apply dropout\n",
        "            conv_input = self.dropout(conv_input)\n",
        "        \n",
        "            #need to pad so decoder can't \"cheat\".. \n",
        "            padding = torch.zeros(batch_size, \n",
        "                                  hid_dim, \n",
        "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
        "            # adding paddings in the beginning of target sentences    \n",
        "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
        "        \n",
        "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(padded_conv_input)\n",
        "\n",
        "            #conved = [batch size, 2 * hid dim, trg len]\n",
        "            \n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim = 1)\n",
        "\n",
        "            #conved = [batch size, hid dim, trg len]\n",
        "            \n",
        "            #calculate attention\n",
        "            attention, conved = self.calculate_attention(embedded, \n",
        "                                                         conved, \n",
        "                                                         encoder_conved, \n",
        "                                                         encoder_combined)\n",
        "            \n",
        "            #attention = [batch size, trg len, src len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "            \n",
        "            #conved = [batch size, hid dim, trg len]\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration\n",
        "            conv_input = conved\n",
        "            \n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "         \n",
        "        #conved = [batch size, trg len, emb dim]\n",
        "            \n",
        "        output = self.fc_out(self.dropout(conved))\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEflNYHsZVn0"
      },
      "source": [
        "Seq2Seq architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFGHH-AyZYld"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n",
        "           \n",
        "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n",
        "        #encoder_conved is output from final encoder conv. block\n",
        "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n",
        "        #  positional embeddings \n",
        "        encoder_conved, encoder_combined = self.encoder(src)\n",
        "            \n",
        "        #encoder_conved = [batch size, src len, emb dim]\n",
        "        #encoder_combined = [batch size, src len, emb dim]\n",
        "        \n",
        "        #calculate predictions of next words\n",
        "        #output is a batch of predictions for each word in the trg sentence\n",
        "        #attention a batch of attention scores across the src sentence for \n",
        "        #  each word in the trg sentence\n",
        "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
        "        \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #attention = [batch size, trg len - 1, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k1Kq0VHZbQ9"
      },
      "source": [
        "Intializing everything:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB4xZj_EZd5-"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
        "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
        "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
        "ENC_KERNEL_SIZE = 3 # must be odd!\n",
        "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
        "ENC_DROPOUT = 0.25\n",
        "DEC_DROPOUT = 0.25\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "    \n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\n",
        "\n",
        "model = Seq2Seq(enc, dec).to(device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj-ZLmtiZfPt"
      },
      "source": [
        "Counting number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9XXGljTZikG",
        "outputId": "11ccf0d3-6f5d-4c49-d185-9687693a1792"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 37,351,685 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obh2ZqnCZjI2"
      },
      "source": [
        "Setting optimizer and criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8JNDLr3Zl0n"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGPQHBVMZois"
      },
      "source": [
        "Train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MBEF8ZMZqJY"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH-mT5I0ZrkM"
      },
      "source": [
        "Test loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nrIP945ZtxC"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "        \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuV4KtZoZu3J"
      },
      "source": [
        "Function for epoch timings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OObz4nGNZ0ZC"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVxxYLwwZ06r"
      },
      "source": [
        "Running the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lDat3O_Z4QE",
        "outputId": "89337411-beea-42e9-ed6c-69baf9d52a3c"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 0.1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 58s\n",
            "\tTrain Loss: 4.228 | Train PPL:  68.581\n",
            "\t Val. Loss: 2.982 |  Val. PPL:  19.720\n",
            "Epoch: 02 | Time: 1m 2s\n",
            "\tTrain Loss: 3.064 | Train PPL:  21.403\n",
            "\t Val. Loss: 2.368 |  Val. PPL:  10.671\n",
            "Epoch: 03 | Time: 1m 3s\n",
            "\tTrain Loss: 2.638 | Train PPL:  13.987\n",
            "\t Val. Loss: 2.151 |  Val. PPL:   8.597\n",
            "Epoch: 04 | Time: 1m 2s\n",
            "\tTrain Loss: 2.410 | Train PPL:  11.131\n",
            "\t Val. Loss: 2.031 |  Val. PPL:   7.619\n",
            "Epoch: 05 | Time: 1m 2s\n",
            "\tTrain Loss: 2.263 | Train PPL:   9.615\n",
            "\t Val. Loss: 1.943 |  Val. PPL:   6.981\n",
            "Epoch: 06 | Time: 1m 2s\n",
            "\tTrain Loss: 2.168 | Train PPL:   8.742\n",
            "\t Val. Loss: 1.886 |  Val. PPL:   6.591\n",
            "Epoch: 07 | Time: 1m 2s\n",
            "\tTrain Loss: 2.090 | Train PPL:   8.086\n",
            "\t Val. Loss: 1.843 |  Val. PPL:   6.316\n",
            "Epoch: 08 | Time: 1m 2s\n",
            "\tTrain Loss: 2.035 | Train PPL:   7.652\n",
            "\t Val. Loss: 1.830 |  Val. PPL:   6.233\n",
            "Epoch: 09 | Time: 1m 2s\n",
            "\tTrain Loss: 1.994 | Train PPL:   7.345\n",
            "\t Val. Loss: 1.807 |  Val. PPL:   6.092\n",
            "Epoch: 10 | Time: 1m 2s\n",
            "\tTrain Loss: 1.955 | Train PPL:   7.063\n",
            "\t Val. Loss: 1.796 |  Val. PPL:   6.024\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
