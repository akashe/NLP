{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English to German Seq2Seq Attn with seq lengths.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbsJIAqrrvwtYfgS0QtW5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashe/NLP/blob/main/assignment/English_to_German_Seq2Seq_Attn_with_seq_lengths.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuZbE99gRX0n"
      },
      "source": [
        "Code for English to German translation using Seq2Seq architecture with packed sequences, attention masking and BLEU scores. Note: the german translations are in reverse order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0x4-zVucW6A",
        "outputId": "2ba5ee7d-27b0-42bd-ca66-167fef16106e"
      },
      "source": [
        "!pip install torchtext==0.5.0 "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (0.1.94)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.5.0) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.5.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM16vdVMgYSt"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import os, pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mCy1ejhgaB6",
        "outputId": "d6b07f40-7f3b-45d4-e0b2-fa10569c58d5"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en\n",
        "python -m spacy download de"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.1.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHVxY3t6gcEc"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29N0rMfqgcoR"
      },
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aYm5LQsgfz8"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGfjLGx9glEt"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True,\n",
        "            include_lengths = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ7IUn8-grFW"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.en', '.de'), \n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QucZ1Mngs8C",
        "outputId": "8a9cc0cc-07fb-4237-d663-1c46db41e166"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds25uRe6gtqG"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZNmYd_2gvFD"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf8SD4nygw00"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, sort_key=lambda x: len(x.src),sort_within_batch=True,\n",
        "    device = device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL1zG0ghgzAo"
      },
      "source": [
        "with open('source.pkl', 'wb') as src_tokens: \n",
        "  pickle.dump(SRC.vocab.stoi, src_tokens)\n",
        "with open('target_stoi.pkl','wb') as trg_tokens:\n",
        "  pickle.dump(TRG.vocab.stoi,trg_tokens)\n",
        "\n",
        "with open('target_itos.pkl','wb') as trg_tokens:\n",
        "  pickle.dump(TRG.vocab.itos,trg_tokens)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sje8iM0fg78h"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src,src_len):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded,src_len.cpu())\n",
        "        # packed_embedded: data_sequence[], batch_sequence[]\n",
        "\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "\n",
        "        outputs,_ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "                \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEfVmaFyLOEc"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs,mask):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "\n",
        "        #attention= [batch size, src len]\n",
        "\n",
        "        attention = attention.masked_fill(mask==0,-1e10)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgmJeWwnMA_H"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs,mask):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs,mask)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgoGg0nDMQis"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device,src_pad_idx):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "\n",
        "    def create_mask(self,src):\n",
        "        mask = (src != self.src_pad_idx).permute(1,0)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src,src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "        # mask: [batch_size,src_len]\n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs,mask)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vc3fqU3M59t"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "src_pad_idx = SRC.vocab.stoi[SRC.pad_token]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device,src_pad_idx).to(device)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3KZ3v46OuHM",
        "outputId": "dbd73e5b-0122-4b75-cfb2-edac382c3825"
      },
      "source": [
        "def init_weights(m):\n",
        "  for name,param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(param.data,mean=0,std=0.01)\n",
        "    else:\n",
        "      nn.init.constant_(param.data,0)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=7855, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd5_8hzGOvqo"
      },
      "source": [
        "pad_idx = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OElJ-vNOyL_"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_xd54QHO0CY",
        "outputId": "8e4062e4-8b3e-4d6c-9c56-0019c5a493d7"
      },
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('Total number of trainable params in the model = {}'.format(count_parameters(model)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of trainable params in the model = 24036783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSr9tBVjO2Qh"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def get_blue_score(translated,translation,itos_location=\"target_itos.pkl\"):\n",
        "  # translated : [trg_len,batch_size,output_dim]\n",
        "  # translation : [trg_len,batch_size]\n",
        "\n",
        "  # Load trg.itos\n",
        "  trg_token_file = open(itos_location,'rb')\n",
        "  trg_itos = pickle.load(trg_token_file)\n",
        "\n",
        "  translated = torch.argmax(translated[1:],-1).T # translated : [batch_size,trg_len-1]\n",
        "  translation = translation[1:].T # translation : [batch_size,trg_len-1]\n",
        "\n",
        "  pred_ = []\n",
        "  trg_ = []\n",
        "  for i,j in zip(translated,translation):\n",
        "    pred_.append([trg_itos[k.item()] for k in i if trg_itos[k.item()] not in ('<eos>','<pad>')]) # trim pad and eos tokens\n",
        "    trg_.append([[trg_itos[k.item()] for k in j if trg_itos[k.item()] not in ('<eos>','<pad>')]])\n",
        "\n",
        "  try:\n",
        "    score = bleu_score(pred_,trg_)\n",
        "    return score\n",
        "  except IndexError:\n",
        "    # My guess is that while doing split with split(\" \"), we are getting ngram len greater than 4\n",
        "    # Find blue_score for each translated and translation sepearately\n",
        "    \n",
        "    # for i,j in zip(translated,translation):\n",
        "    #   try:\n",
        "    #     bleu_score([[trg_itos[k.item()] for k in i]],[[[trg_itos[k.item()] for k in j]]])\n",
        "    #   except IndexError:\n",
        "    #     print(f' Translated result {[trg_itos[k.item()] for k in i]}')\n",
        "    #     print(f' Target Result {[trg_itos[k.item()] for k in j]}')\n",
        "\n",
        "    '''\n",
        "    The speculation was right. When the translated sentence has a ' ' in it. Then the length of ngram increases to 5 after splitting it string.split(\" \")\n",
        "    For now simply passing 0 score if this error is encountered.\n",
        "    '''\n",
        "    return 0"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BwZm6c8QMQz"
      },
      "source": [
        "def train(model,iterator,optimizer,criterion,clip):\n",
        "  model.train()\n",
        "\n",
        "  epoch_loss = 0\n",
        "  bleu_score = 0\n",
        "  for i,batch in enumerate(iterator):\n",
        "\n",
        "    src,src_len = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(src,src_len,trg)\n",
        "\n",
        "    #outputs : [trg_len,batch,output_dim]\n",
        "    # trg :[trg_len,batch]\n",
        "\n",
        "    output_dim = outputs.shape[-1]\n",
        "\n",
        "    batch_bleu_score = get_blue_score(outputs,trg)\n",
        "    bleu_score += batch_bleu_score\n",
        "\n",
        "    outputs = outputs[1:].view(-1,output_dim)\n",
        "    trg = trg[1:].view(-1)\n",
        "\n",
        "    loss = criterion(outputs,trg)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm(model.parameters(),clip)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  \n",
        "  return epoch_loss/len(iterator) , bleu_score/len(iterator)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp7i1rycQZqi"
      },
      "source": [
        "def evaluate(model,criterion,iterator):\n",
        "  model.eval()\n",
        "\n",
        "  epoch_loss = 0\n",
        "  bleu_score = 0\n",
        "  for i,batch in enumerate(iterator):\n",
        "\n",
        "    src,src_len = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    outputs = model(src,src_len,trg)\n",
        "\n",
        "    output_dim = outputs.shape[-1]\n",
        "\n",
        "    batch_bleu_score = get_blue_score(outputs,trg)\n",
        "    bleu_score += batch_bleu_score\n",
        "\n",
        "    outputs = outputs[1:].view(-1,output_dim)\n",
        "    trg = trg[1:].view(-1)\n",
        "\n",
        "    loss = criterion(outputs,trg)\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss/len(iterator) ,bleu_score/len(iterator)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yeXlfrJQbm_"
      },
      "source": [
        "def epoch_time(start_time,end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time/60)\n",
        "  elapsed_secs = int(elapsed_time - elapsed_mins*60)\n",
        "  return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTCgG3ixQutt",
        "outputId": "cfafdd90-1dde-4b7b-9cc7-89de530f9a22"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "epochs = 20\n",
        "clip = 1.\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  train_loss,train_bleu = train(model,train_iterator,optimizer,criterion,clip)\n",
        "  val_loss,val_bleu = evaluate(model,criterion,valid_iterator)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  if best_valid_loss > val_loss:\n",
        "    best_valid_loss = val_loss\n",
        "    torch.save(model.state_dict(),'english-german-seq2seq.pt')\n",
        "  \n",
        "  epoch_mins,epoch_secs = epoch_time(start_time,end_time)\n",
        "\n",
        "  print(f'Epoch : {epoch+1:02} | Time: {epoch_mins:.3f} minutes ,{epoch_secs} seconds')\n",
        "  print(f'\\t Train Loss : {train_loss:.3f} | Train BLEU : {train_bleu:.4f} | Train PPL: {math.exp(train_loss):7.3f} ')\n",
        "  print(f'\\t Val loss : {val_loss:.3f} | Val BLEU: {val_bleu:.4f} |Val PPL: {math.exp(val_loss):7.3f}')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 01 | Time: 1.000 minutes ,41 seconds\n",
            "\t Train Loss : 4.073 | Train BLEU : 0.0400 | Train PPL:  58.714 \n",
            "\t Val loss : 3.616 | Val BLEU: 0.0830 |Val PPL:  37.176\n",
            "Epoch : 02 | Time: 1.000 minutes ,45 seconds\n",
            "\t Train Loss : 3.401 | Train BLEU : 0.0954 | Train PPL:  29.989 \n",
            "\t Val loss : 3.164 | Val BLEU: 0.1371 |Val PPL:  23.666\n",
            "Epoch : 03 | Time: 1.000 minutes ,48 seconds\n",
            "\t Train Loss : 2.903 | Train BLEU : 0.1394 | Train PPL:  18.234 \n",
            "\t Val loss : 2.868 | Val BLEU: 0.1648 |Val PPL:  17.601\n",
            "Epoch : 04 | Time: 1.000 minutes ,46 seconds\n",
            "\t Train Loss : 2.508 | Train BLEU : 0.1768 | Train PPL:  12.276 \n",
            "\t Val loss : 2.542 | Val BLEU: 0.1729 |Val PPL:  12.703\n",
            "Epoch : 05 | Time: 1.000 minutes ,48 seconds\n",
            "\t Train Loss : 2.189 | Train BLEU : 0.2149 | Train PPL:   8.923 \n",
            "\t Val loss : 2.541 | Val BLEU: 0.2061 |Val PPL:  12.698\n",
            "Epoch : 06 | Time: 1.000 minutes ,49 seconds\n",
            "\t Train Loss : 1.886 | Train BLEU : 0.2636 | Train PPL:   6.590 \n",
            "\t Val loss : 2.565 | Val BLEU: 0.2174 |Val PPL:  12.995\n",
            "Epoch : 07 | Time: 1.000 minutes ,50 seconds\n",
            "\t Train Loss : 1.687 | Train BLEU : 0.3036 | Train PPL:   5.401 \n",
            "\t Val loss : 2.504 | Val BLEU: 0.2076 |Val PPL:  12.232\n",
            "Epoch : 08 | Time: 1.000 minutes ,51 seconds\n",
            "\t Train Loss : 1.533 | Train BLEU : 0.3323 | Train PPL:   4.630 \n",
            "\t Val loss : 2.533 | Val BLEU: 0.1977 |Val PPL:  12.594\n",
            "Epoch : 09 | Time: 1.000 minutes ,51 seconds\n",
            "\t Train Loss : 1.402 | Train BLEU : 0.3633 | Train PPL:   4.064 \n",
            "\t Val loss : 2.493 | Val BLEU: 0.2330 |Val PPL:  12.096\n",
            "Epoch : 10 | Time: 1.000 minutes ,51 seconds\n",
            "\t Train Loss : 1.266 | Train BLEU : 0.3919 | Train PPL:   3.545 \n",
            "\t Val loss : 2.478 | Val BLEU: 0.2275 |Val PPL:  11.912\n",
            "Epoch : 11 | Time: 1.000 minutes ,51 seconds\n",
            "\t Train Loss : 1.177 | Train BLEU : 0.4254 | Train PPL:   3.245 \n",
            "\t Val loss : 2.708 | Val BLEU: 0.1971 |Val PPL:  15.002\n",
            "Epoch : 12 | Time: 1.000 minutes ,49 seconds\n",
            "\t Train Loss : 1.076 | Train BLEU : 0.4464 | Train PPL:   2.934 \n",
            "\t Val loss : 2.764 | Val BLEU: 0.2234 |Val PPL:  15.859\n",
            "Epoch : 13 | Time: 1.000 minutes ,52 seconds\n",
            "\t Train Loss : 0.980 | Train BLEU : 0.4797 | Train PPL:   2.664 \n",
            "\t Val loss : 2.640 | Val BLEU: 0.2365 |Val PPL:  14.014\n",
            "Epoch : 14 | Time: 1.000 minutes ,52 seconds\n",
            "\t Train Loss : 0.913 | Train BLEU : 0.4911 | Train PPL:   2.491 \n",
            "\t Val loss : 2.657 | Val BLEU: 0.2260 |Val PPL:  14.258\n",
            "Epoch : 15 | Time: 1.000 minutes ,52 seconds\n",
            "\t Train Loss : 0.846 | Train BLEU : 0.5186 | Train PPL:   2.330 \n",
            "\t Val loss : 2.826 | Val BLEU: 0.2467 |Val PPL:  16.870\n",
            "Epoch : 16 | Time: 1.000 minutes ,53 seconds\n",
            "\t Train Loss : 0.757 | Train BLEU : 0.5226 | Train PPL:   2.132 \n",
            "\t Val loss : 2.683 | Val BLEU: 0.2452 |Val PPL:  14.636\n",
            "Epoch : 17 | Time: 1.000 minutes ,56 seconds\n",
            "\t Train Loss : 0.699 | Train BLEU : 0.5373 | Train PPL:   2.012 \n",
            "\t Val loss : 2.746 | Val BLEU: 0.2298 |Val PPL:  15.587\n",
            "Epoch : 18 | Time: 1.000 minutes ,55 seconds\n",
            "\t Train Loss : 0.681 | Train BLEU : 0.5381 | Train PPL:   1.975 \n",
            "\t Val loss : 2.837 | Val BLEU: 0.2095 |Val PPL:  17.064\n",
            "Epoch : 19 | Time: 1.000 minutes ,55 seconds\n",
            "\t Train Loss : 0.631 | Train BLEU : 0.5636 | Train PPL:   1.880 \n",
            "\t Val loss : 2.941 | Val BLEU: 0.2136 |Val PPL:  18.938\n",
            "Epoch : 20 | Time: 1.000 minutes ,56 seconds\n",
            "\t Train Loss : 0.580 | Train BLEU : 0.6003 | Train PPL:   1.786 \n",
            "\t Val loss : 3.206 | Val BLEU: 0.2046 |Val PPL:  24.677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLDsH2vJQwoU",
        "outputId": "18f3c162-6ec7-458d-9454-eb1c012f104a"
      },
      "source": [
        "# Testing the model\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "best_model_at = 'english-german-seq2seq.pt'\n",
        "model.load_state_dict(torch.load(best_model_at))\n",
        "model.eval()\n",
        "\n",
        "# src stoi vocab\n",
        "src_token_file = open(\"source.pkl\",'rb')\n",
        "src_vocab = pickle.load(src_token_file)\n",
        "\n",
        "\n",
        "# target stoi vocab\n",
        "trg_tokens_file = open('target_stoi.pkl','rb')\n",
        "trg_stoi = pickle.load(trg_tokens_file)\n",
        "\n",
        "# target itos vocab\n",
        "trg_token_file = open(\"target_itos.pkl\",'rb')\n",
        "trg_itos = pickle.load(trg_token_file)\n",
        "\n",
        "\n",
        "# Creating src tensor\n",
        "custom_src_ = \"A boy was swimming in the river.\"\n",
        "tokenized_custom_src = [i.text.lower() for i in nlp.tokenizer(custom_src_)]\n",
        "print(tokenized_custom_src)\n",
        "tokenized_custom_src = ['<sos>'] + tokenized_custom_src +['<eos>']\n",
        "custom_src = [src_vocab[i] for i in tokenized_custom_src]\n",
        "custom_src = torch.LongTensor(custom_src).to(device).unsqueeze(1)\n",
        "custom_src_len = torch.LongTensor([len(custom_src)]).to(device)\n",
        "\n",
        "# Creating trg tensor\n",
        "eval_trg_len = 15\n",
        "trg_seq = [trg_stoi['<sos>']]\n",
        "# There are two option to proceed. Pass src and trg to model.forward() or step by step model.encoder() and model.decoder(). With the second\n",
        "# you can visualize attention\n",
        "\n",
        "with torch.no_grad():\n",
        "  encoder_outputs, hidden = model.encoder(custom_src,custom_src_len)\n",
        "\n",
        "mask = model.create_mask(custom_src)\n",
        "\n",
        "attentions = []\n",
        "outputs = []\n",
        "\n",
        "for i in range(eval_trg_len): # Could have used range(eval_trg_len)\n",
        "  with torch.no_grad():\n",
        "    trg_tensor = torch.LongTensor([trg_seq[-1]]).to(device)\n",
        "    output, hidden,attention = model.decoder(trg_tensor,hidden,encoder_outputs,mask)\n",
        "  attentions.append(attention)\n",
        "  pred_token = output.argmax(1).item()\n",
        "  trg_seq.append(pred_token)\n",
        "\n",
        "  if pred_token == trg_stoi['<eos>']:\n",
        "    break\n",
        "\n",
        "attentions = torch.cat(attentions).to(device)\n",
        "translation = [trg_itos[i] for i in trg_seq]\n",
        "translation_joined = \" \".join(trg_itos[i] for i in trg_seq[1:-1][::-1])\n",
        "print(f\" German translation : {translation_joined}\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'boy', 'was', 'swimming', 'in', 'the', 'river', '.']\n",
            " German translation : ein junge , der in im fluss schwimmt .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "GkNlzfquQz7f",
        "outputId": "48a6a7d7-69f7-419b-9ca2-f74d1543313c"
      },
      "source": [
        "# Printing Attention vector\n",
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.ticker as ticker\n",
        "def display_attention(sentence,translation,attention):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(111)\n",
        "\n",
        "  attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "\n",
        "  cax = ax.matshow(attention, cmap='bone')\n",
        "\n",
        "  ax.tick_params(labelsize=15)\n",
        "  ax.set_xticklabels(['']+[t for t in tokenized_custom_src],rotation=45)\n",
        "  ax.set_yticklabels(['']+ [t for t in translation])\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "display_attention(custom_src_,translation,attentions)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}