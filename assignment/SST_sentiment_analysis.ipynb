{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SST sentiment analysis",
      "provenance": [],
      "authorship_tag": "ABX9TyODg7h7kVl4cH63ZbbUQ+yP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashe/NLP/blob/main/assignment/SST_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU1vCbBTftFR"
      },
      "source": [
        "In this notebook, we will do sentiment analysis on Standford sentiment Treebank dataset. \n",
        "The core things-\n",
        "1) We will focussing on sentiments of the phrases with 5 classes.\n",
        "2) data augmentation\n",
        "3) model architectures.\n",
        "\n",
        "Original paper: https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf\n",
        "Dataset location: https://nlp.stanford.edu/sentiment/\n",
        "We wont make the same architecture as mentioned in the paper. We will use a simple LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vOhYGpRR4Kt"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYliYJ3cThaU"
      },
      "source": [
        "# Download packages\n",
        "!pip install zipfile > /dev/null 2>&1  \n",
        "!pip install io > /dev/null 2>&1  "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJjbfGhXR9dQ"
      },
      "source": [
        "# Download dataset\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile \n",
        "from io import BytesIO\n",
        "import random\n",
        "\n",
        "dataset_location = \"http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\"\n",
        "zipresp = urlopen(dataset_location)\n",
        "with urlopen(dataset_location) as zipresp:\n",
        "  with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
        "    zfile.extractall(\"./SST_dataset\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "305Ux4_bVZQr",
        "outputId": "06b21d4a-f575-4a75-f75e-e2fc08cf51ca"
      },
      "source": [
        "# Loading data\n",
        "# phrases are present in dictionary.txt\n",
        "\n",
        "phrases = {}\n",
        "with open(\"/content/SST_dataset/stanfordSentimentTreebank/dictionary.txt\") as f:\n",
        "  for line in f:\n",
        "    phrase,id = line.strip(\"\\n\").split(\"|\")\n",
        "    phrases[id]= {}\n",
        "    phrases[id][\"phrase\"]= phrase\n",
        "\n",
        "with open(\"/content/SST_dataset/stanfordSentimentTreebank/sentiment_labels.txt\") as f:\n",
        "  i = 0\n",
        "  for line in f:\n",
        "    if i==0: i+=1;continue\n",
        "    id,sentiment = line.strip(\"\\n\").split(\"|\")\n",
        "    if id not in phrases:\n",
        "      raise(KeyError)\n",
        "    else:\n",
        "      phrases[id][\"sentiment\"] = sentiment\n",
        "\n",
        "print(phrases['0'],phrases['25'])  \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'phrase': '!', 'sentiment': '0.5'} {'phrase': \"'s a visual delight and a decent popcorn adventure ,\", 'sentiment': '0.77778'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3BddDhMdvR-"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UhDBxaadi5o"
      },
      "source": [
        "# #Back Translate\n",
        "# # Install package for google translate\n",
        "# # using google_trans_new for unlimited translations\n",
        "# !pip install google_trans_new > /dev/null 2>&1\n",
        "# !pip install googletrans > /dev/null 2>&1\n",
        "\n",
        "\n",
        "# from google_trans_new import google_translator\n",
        "# import random\n",
        "# import googletrans\n",
        "\n",
        "# translator = google_translator()\n",
        "# back_translated_phrases = {}\n",
        "\n",
        "# for i in phrases:\n",
        "#   sentence = phrases[i]['phrase']\n",
        "#   available_langs = list(googletrans.LANGUAGES.keys())\n",
        "#   trans_lang = random.choice(available_langs) \n",
        "#   translation = translator.translate(sentence,lang_tgt=trans_lang)\n",
        "#   assert type(translation) is str \n",
        "#   back_translation = translator.translate(translation, lang_tgt=\"en\")\n",
        "#   if sentence == back_translation:\n",
        "#     continue\n",
        "#   else:\n",
        "#     new_id = str(len(back_translated_phrases))\n",
        "#     back_translated_phrases[new_id] = {}\n",
        "#     back_translated_phrases[new_id]['phrase']= back_translation\n",
        "#     back_translated_phrases[new_id]['sentiment'] = phrases[i]['sentiment']\n",
        "\n",
        "# print(\"Number of datapoints added after back translation {}\".format(len(back_translated_phrases)))\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP9cueXni7gD"
      },
      "source": [
        "# Dump back translated sentence for future use:\n",
        "!pip install pickle > /dev/null 2>&1\n",
        "import pickle\n",
        "\n",
        "# with open(\"/content/SST_dataset/back_translated.pickle\",\"wb\") as f:\n",
        "#   pickle.dump(back_translated_phrases,f)\n",
        "\n",
        "# Also dump existing data for later use\n",
        "with open(\"/content/SST_dataset/phrases.pickle\",\"wb\") as f:\n",
        "  pickle.dump(phrases,f)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0UdV-rgeDPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842e131f-6abe-4f01-c5c2-aaba02acb66a"
      },
      "source": [
        "#random_swap\n",
        "# Swapping words within a phrase for n times\n",
        "n = 5\n",
        "\n",
        "def random_swap(sentence):\n",
        "  sentence = sentence.split(\" \")\n",
        "  if len(sentence)<3:\n",
        "    return \" \".join(sentence)\n",
        "  length = range(len(sentence))\n",
        "  for _ in range(n):\n",
        "    idx1, idx2 = random.sample(length,2)\n",
        "    sentence[idx1],sentence[idx2] = sentence[idx2],sentence[idx1]\n",
        "  return \" \".join(sentence)\n",
        "\n",
        "random_swapped_phrases = {}\n",
        "\n",
        "for i in phrases:\n",
        "  sentence = phrases[i]['phrase']\n",
        "  swapped_sentence = random_swap(sentence)\n",
        "  if sentence != swapped_sentence:\n",
        "    new_idx = str(len(random_swapped_phrases))\n",
        "    random_swapped_phrases[new_idx] = {}\n",
        "    random_swapped_phrases[new_idx]['phrase'] = swapped_sentence\n",
        "    random_swapped_phrases[new_idx]['sentiment'] = phrases[i]['sentiment']\n",
        "\n",
        "print(\" Number of data points added with random swap = {}\".format(len(random_swapped_phrases)))\n",
        "\n",
        "with open(\"/content/SST_dataset/swapped_phrases.pickle\",\"wb\") as f:\n",
        "  pickle.dump(random_swapped_phrases,f)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Number of data points added with random swap = 179306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZCGrlJ0eKD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7fc4418-0a6d-4b74-d7ed-96c45cb29548"
      },
      "source": [
        "# random_delete\n",
        "# Deleting a word from sentences with a probablilty greater than p\n",
        "\n",
        "p = 0.8\n",
        "\n",
        "def random_deletion(sentence):\n",
        "  sentence = sentence.split(\" \")\n",
        "  if len(sentence) == 1:\n",
        "    return \" \".join(sentence)\n",
        "  pruned_sentence = list(filter(lambda x: random.uniform(0,1)>p,sentence))\n",
        "  if len(pruned_sentence)==0:\n",
        "    return random.choice(sentence)\n",
        "  else:\n",
        "    return \" \".join(pruned_sentence)\n",
        "\n",
        "random_deleted_phrases = {}\n",
        "\n",
        "for i in phrases:\n",
        "  sentence = phrases[i]['phrase']\n",
        "  deleted_sentence = random_deletion(sentence)\n",
        "  if sentence != deleted_sentence:\n",
        "    new_idx = str(len(random_deleted_phrases))\n",
        "    random_deleted_phrases[new_idx] = {}\n",
        "    random_deleted_phrases[new_idx]['phrase'] = deleted_sentence\n",
        "    random_deleted_phrases[new_idx]['sentiment'] = phrases[i]['sentiment']\n",
        "\n",
        "print(\" Number of data points added with random deletion = {}\".format(len(random_deleted_phrases)))\n",
        "\n",
        "with open(\"/content/SST_dataset/deleted_phrases.pickle\",\"wb\") as f:\n",
        "  pickle.dump(random_deleted_phrases,f)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Number of data points added with random deletion = 215095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCw6mGbZ9D3l"
      },
      "source": [
        "# Defining Fields, Datasets and (train,valid) splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeYBlyRR9Xuv"
      },
      "source": [
        "!pip install torch > /dev/null 2>&1\n",
        "\n",
        "import torch, torchtext\n",
        "from torchtext import data\n",
        "\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "phrase = data.Field(sequential = True, tokenize = 'spacy', batch_first = True, include_lengths = True)\n",
        "sentiment = data.LabelField(tokenize= 'spacy', is_target = True, preprocessing = lambda x: int(float(x)/0.2), batch_first = True, sequential = False)\n",
        "\n",
        "fields = [('phrase',phrase),('sentiment',sentiment)]\n",
        "\n",
        "def dict_custom_add(*args):\n",
        "  # A simpler way would be c = {**a,**b} but that would override same key values\n",
        "  new_dict = {}\n",
        "  for i in args:\n",
        "    assert type(i) is dict\n",
        "    for j in i:\n",
        "      if j not in new_dict:\n",
        "        new_dict[j] = i[j]\n",
        "      else:\n",
        "        new_dict[str(len(new_dict))] = i[j]\n",
        "\n",
        "  return new_dict\n",
        "\n",
        "# For albation studies, we will make 5 seperate datasets\n",
        "\n",
        "# TODO: convert creating train and valid for these datasets into a loop\n",
        "\n",
        "# 1. Orignal data:\n",
        "original_examples = [data.Example.fromlist([phrases[i]['phrase'],phrases[i]['sentiment']],fields) for i in phrases]\n",
        "original_dataset = data.Dataset(original_examples,fields)\n",
        "\n",
        "original_train, original_valid = original_dataset.split(split_ratio=[0.85,0.15], random_state= random.seed(seed))\n",
        "\n",
        "# # 2. Back translated data:\n",
        "# custom_dict = dict_custom_add(phrases,back_translated_phrases)\n",
        "# back_translated_examples = [data.Example.fromlist([custom_dict[i]['phrase'],custom_dict[i]['sentiment']],fields) for i in custom_dict]\n",
        "# back_translated_dataset = data.Dataset(back_translated_examples,fields)\n",
        "\n",
        "# back_translate_train, back_translated_valid = back_translated_dataset.split(split_ratio=[0.85,0.15], random_state= random.seed(seed))\n",
        "\n",
        "# 3. Random swapped data:\n",
        "custom_dict = dict_custom_add(phrases,random_swapped_phrases)\n",
        "random_swapped_examples = [data.Example.fromlist([custom_dict[i]['phrase'],custom_dict[i]['sentiment']],fields) for i in custom_dict]\n",
        "random_swapped_dataset = data.Dataset(random_swapped_examples,fields)\n",
        "\n",
        "random_swapped_train, random_swapped_valid = random_swapped_dataset.split(split_ratio=[0.85,0.15], random_state= random.seed(seed))\n",
        "\n",
        "# 4. Random deletion data:\n",
        "custom_dict = dict_custom_add(phrases,random_deleted_phrases)\n",
        "random_deletion_examples = [data.Example.fromlist([custom_dict[i]['phrase'],custom_dict[i]['sentiment']],fields) for i in custom_dict]\n",
        "random_deletion_dataset = data.Dataset(random_deletion_examples,fields)\n",
        "\n",
        "random_deletion_train, random_deletion_valid = random_deletion_dataset.split(split_ratio=[0.85,0.15], random_state= random.seed(seed))\n",
        "\n",
        "# 5. All the above combined:\n",
        "custom_dict = dict_custom_add(phrases,random_swapped_phrases, random_deleted_phrases)\n",
        "full_exmaples = [data.Example.fromlist([custom_dict[i]['phrase'],custom_dict[i]['sentiment']],fields) for i in custom_dict]\n",
        "full_dataset = data.Dataset(full_exmaples,fields)\n",
        "\n",
        "full_train, full_valid = full_dataset.split(split_ratio=[0.85,0.15], random_state= random.seed(seed))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb8FzXcdInW4"
      },
      "source": [
        "# Defining model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdnbFFO6IrEY"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcv75_s2Joer"
      },
      "source": [
        "# Train Eval Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn5lLm1HJuN0"
      },
      "source": [
        "# Set device\n",
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrhOYR5JJ5R2"
      },
      "source": [
        "def runner(train_dataset,valid_dataset,epochs=10,embedding_dim=300,num_hidden_nodes=100,num_layers=2,dropout=0.2,lr=2e-4):\n",
        "  # build vocab\n",
        "  phrase.build_vocab(train_dataset)\n",
        "  sentiment.build_vocab(train_dataset)\n",
        "\n",
        "  # build iterators\n",
        "  train_iterator,valid_iterator = data.BucketIterator.splits((train_dataset, valid_dataset), batch_size = 32, \n",
        "                                                            sort_key = lambda x: len(x.phrase),\n",
        "                                                            sort_within_batch=True, device = device)\n",
        "  \n",
        "  # intialize models\n",
        "  size_of_vocab = len(phrase.vocab)\n",
        "  num_output_nodes = len(sentiment.vocab)\n",
        "  model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)\n",
        "  \n",
        "  # optimizer\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  \n",
        "  # accuracy\n",
        "  def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "  \n",
        "  # move model and crtierion to gpu if available\n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  # train loop\n",
        "  def train():\n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in train_iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        phrase, phrase_lengths = batch.phrase   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(phrase, phrase_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.sentiment)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.sentiment)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(train_iterator), epoch_acc / len(train_iterator)\n",
        "\n",
        "  # eval loop\n",
        "  def evaluate():\n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in valid_iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            phrase, phrase_lengths = batch.phrase\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(phrase, phrase_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.sentiment)\n",
        "            acc = binary_accuracy(predictions, batch.sentiment)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(valid_iterator), epoch_acc / len(valid_iterator)\n",
        "  \n",
        "  # Running for epochs\n",
        "  best_valid_accuracy = float('-inf')\n",
        "  accuracy = None\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    # train the model\n",
        "    train_loss, train_acc = train()\n",
        "\n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate()\n",
        "\n",
        "    if valid_acc > best_valid_accuracy:\n",
        "      best_valid_accuracy = valid_acc\n",
        "\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')\n",
        "  # return best accuracy\n",
        "  return best_valid_accuracy"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVP8RPvkTBRo"
      },
      "source": [
        "\n",
        "# Training models for all datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H58IUHXWTIN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "437f2465-84ce-4663-a5d2-a5e4b275451d"
      },
      "source": [
        "best_accuracies = []\n",
        "print(\"Training with Original data\")\n",
        "best_accuracy = runner(original_train,original_valid)\n",
        "print(\"Best accuracy with Original data {}\".format(best_accuracy))\n",
        "best_accuracies.append((\"Orginal data -->\",best_accuracy))\n",
        "\n",
        "# print(\"Training with Original data + Back Translation data\")\n",
        "# best_accuracy = runner(back_translate_train,back_translated_valid)\n",
        "# print(\"Best accuracy with Original data + Back Translation data {}\".format(best_accuracy))\n",
        "# best_accuracies.append((\"Original data + Back Translation data -->\",best_accuracy))\n",
        "\n",
        "print(\"\\nTraining with Original data + Random Swapped data\")\n",
        "best_accuracy = runner(random_swapped_train,random_swapped_valid)\n",
        "print(\"Best accuracy with Original data + Random Swapped data {}\".format(best_accuracy))\n",
        "best_accuracies.append((\"Original data + Random Swapped data -->\",best_accuracy))\n",
        "\n",
        "print(\"\\nTraining with Original data + Random Deleted data\")\n",
        "best_accuracy = runner(random_deletion_train,random_deletion_valid)\n",
        "print(\"Best accuracy with Original data + Random Deleted data {}\".format(best_accuracy))\n",
        "best_accuracies.append((\"Original data + Random Deleted data -->\",best_accuracy))\n",
        "\n",
        "print(\"\\nTraining with Original data + Random Swapped data + Random Deleted data\")\n",
        "best_accuracy = runner(full_train,full_valid)\n",
        "print(\"Best accuracy with Original data + Random Swapped data + Random Deleted data {}\".format(best_accuracy))\n",
        "best_accuracies.append((\"Original data + Random Swapped data + Random Deleted data -->\",best_accuracy))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with Original data\n",
            "\tTrain Loss: 1.533 | Train Acc: 51.59%\n",
            "\t Val. Loss: 1.492 |  Val. Acc: 54.72% \n",
            "\n",
            "\tTrain Loss: 1.462 | Train Acc: 57.95%\n",
            "\t Val. Loss: 1.453 |  Val. Acc: 58.93% \n",
            "\n",
            "\tTrain Loss: 1.426 | Train Acc: 61.69%\n",
            "\t Val. Loss: 1.435 |  Val. Acc: 60.69% \n",
            "\n",
            "\tTrain Loss: 1.402 | Train Acc: 64.11%\n",
            "\t Val. Loss: 1.427 |  Val. Acc: 61.48% \n",
            "\n",
            "\tTrain Loss: 1.385 | Train Acc: 65.92%\n",
            "\t Val. Loss: 1.420 |  Val. Acc: 62.20% \n",
            "\n",
            "\tTrain Loss: 1.372 | Train Acc: 67.19%\n",
            "\t Val. Loss: 1.415 |  Val. Acc: 62.78% \n",
            "\n",
            "\tTrain Loss: 1.362 | Train Acc: 68.27%\n",
            "\t Val. Loss: 1.413 |  Val. Acc: 63.00% \n",
            "\n",
            "\tTrain Loss: 1.353 | Train Acc: 69.11%\n",
            "\t Val. Loss: 1.410 |  Val. Acc: 63.23% \n",
            "\n",
            "\tTrain Loss: 1.347 | Train Acc: 69.83%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 63.20% \n",
            "\n",
            "\tTrain Loss: 1.340 | Train Acc: 70.46%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 63.35% \n",
            "\n",
            "Best accuracy with Original data 0.6335441519551098\n",
            "\n",
            "Training with Original data + Random Swapped data\n",
            "\tTrain Loss: 1.529 | Train Acc: 51.55%\n",
            "\t Val. Loss: 1.475 |  Val. Acc: 56.68% \n",
            "\n",
            "\tTrain Loss: 1.450 | Train Acc: 59.20%\n",
            "\t Val. Loss: 1.439 |  Val. Acc: 60.27% \n",
            "\n",
            "\tTrain Loss: 1.416 | Train Acc: 62.67%\n",
            "\t Val. Loss: 1.423 |  Val. Acc: 61.80% \n",
            "\n",
            "\tTrain Loss: 1.396 | Train Acc: 64.75%\n",
            "\t Val. Loss: 1.414 |  Val. Acc: 62.88% \n",
            "\n",
            "\tTrain Loss: 1.382 | Train Acc: 66.18%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 63.24% \n",
            "\n",
            "\tTrain Loss: 1.371 | Train Acc: 67.31%\n",
            "\t Val. Loss: 1.405 |  Val. Acc: 63.70% \n",
            "\n",
            "\tTrain Loss: 1.362 | Train Acc: 68.19%\n",
            "\t Val. Loss: 1.404 |  Val. Acc: 63.80% \n",
            "\n",
            "\tTrain Loss: 1.354 | Train Acc: 69.01%\n",
            "\t Val. Loss: 1.401 |  Val. Acc: 64.12% \n",
            "\n",
            "\tTrain Loss: 1.347 | Train Acc: 69.74%\n",
            "\t Val. Loss: 1.398 |  Val. Acc: 64.33% \n",
            "\n",
            "\tTrain Loss: 1.341 | Train Acc: 70.37%\n",
            "\t Val. Loss: 1.398 |  Val. Acc: 64.35% \n",
            "\n",
            "Best accuracy with Original data + Random Swapped data 0.6435084493335234\n",
            "\n",
            "Training with Original data + Random Deleted data\n",
            "\tTrain Loss: 1.558 | Train Acc: 49.03%\n",
            "\t Val. Loss: 1.525 |  Val. Acc: 51.45% \n",
            "\n",
            "\tTrain Loss: 1.510 | Train Acc: 53.01%\n",
            "\t Val. Loss: 1.496 |  Val. Acc: 54.42% \n",
            "\n",
            "\tTrain Loss: 1.479 | Train Acc: 56.27%\n",
            "\t Val. Loss: 1.479 |  Val. Acc: 56.15% \n",
            "\n",
            "\tTrain Loss: 1.458 | Train Acc: 58.44%\n",
            "\t Val. Loss: 1.469 |  Val. Acc: 57.14% \n",
            "\n",
            "\tTrain Loss: 1.443 | Train Acc: 59.95%\n",
            "\t Val. Loss: 1.464 |  Val. Acc: 57.62% \n",
            "\n",
            "\tTrain Loss: 1.432 | Train Acc: 61.09%\n",
            "\t Val. Loss: 1.462 |  Val. Acc: 57.85% \n",
            "\n",
            "\tTrain Loss: 1.423 | Train Acc: 62.00%\n",
            "\t Val. Loss: 1.460 |  Val. Acc: 58.12% \n",
            "\n",
            "\tTrain Loss: 1.416 | Train Acc: 62.80%\n",
            "\t Val. Loss: 1.459 |  Val. Acc: 58.13% \n",
            "\n",
            "\tTrain Loss: 1.409 | Train Acc: 63.50%\n",
            "\t Val. Loss: 1.458 |  Val. Acc: 58.26% \n",
            "\n",
            "\tTrain Loss: 1.403 | Train Acc: 64.12%\n",
            "\t Val. Loss: 1.459 |  Val. Acc: 58.15% \n",
            "\n",
            "Best accuracy with Original data + Random Deleted data 0.5825606416368708\n",
            "\n",
            "Training with Original data + Random Swapped data + Random Deleted data\n",
            "\tTrain Loss: 1.550 | Train Acc: 49.42%\n",
            "\t Val. Loss: 1.503 |  Val. Acc: 53.77% \n",
            "\n",
            "\tTrain Loss: 1.481 | Train Acc: 56.03%\n",
            "\t Val. Loss: 1.471 |  Val. Acc: 57.08% \n",
            "\n",
            "\tTrain Loss: 1.451 | Train Acc: 59.10%\n",
            "\t Val. Loss: 1.457 |  Val. Acc: 58.51% \n",
            "\n",
            "\tTrain Loss: 1.433 | Train Acc: 60.90%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 59.04% \n",
            "\n",
            "\tTrain Loss: 1.421 | Train Acc: 62.17%\n",
            "\t Val. Loss: 1.446 |  Val. Acc: 59.57% \n",
            "\n",
            "\tTrain Loss: 1.412 | Train Acc: 63.13%\n",
            "\t Val. Loss: 1.443 |  Val. Acc: 59.78% \n",
            "\n",
            "\tTrain Loss: 1.403 | Train Acc: 64.00%\n",
            "\t Val. Loss: 1.441 |  Val. Acc: 60.01% \n",
            "\n",
            "\tTrain Loss: 1.397 | Train Acc: 64.73%\n",
            "\t Val. Loss: 1.440 |  Val. Acc: 60.03% \n",
            "\n",
            "\tTrain Loss: 1.391 | Train Acc: 65.33%\n",
            "\t Val. Loss: 1.439 |  Val. Acc: 60.22% \n",
            "\n",
            "\tTrain Loss: 1.385 | Train Acc: 65.87%\n",
            "\t Val. Loss: 1.439 |  Val. Acc: 60.19% \n",
            "\n",
            "Best accuracy with Original data + Random Swapped data + Random Deleted data 0.602231992596089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiiAlF-PWpEF"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4w377amWrS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aca9e2b-984a-4600-bf70-0229bc05d806"
      },
      "source": [
        "print(\" Best Accuracies for different augmentation methods\")\n",
        "for i,j in best_accuracies:\n",
        "  print(str(j*100)+\"\\t\"+i)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Best Accuracies for different augmentation methods\n",
            "63.35441519551098\tOrginal data -->\n",
            "64.35084493335233\tOriginal data + Random Swapped data -->\n",
            "58.256064163687086\tOriginal data + Random Deleted data -->\n",
            "60.2231992596089\tOriginal data + Random Swapped data + Random Deleted data -->\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}