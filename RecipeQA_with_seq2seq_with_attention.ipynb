{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFWTonZWZ3jQ"
   },
   "source": [
    "RecipeQA is a QA dataset with focus on using multimodal information for question answering. In this prelimanary experiment, we set the task as a seq2seq problem so we will focus only on the 'textual_cloze' subproblems and ignoring visual information for now. \n",
    "\n",
    "\n",
    "Each 'textual_cloze' examples has a context which contains different parts of the recipe. We use a custom identifier '@context' to concatenate different parts of context to a single sequence to help in differentiate different parts of context.  \n",
    "\n",
    "Each problem has a question to \"Choose the best title for the missing blank to correctly complete the recipe.\" with recipe shown as \n",
    "\n",
    "\"['Ingredients Halal Vanilla Extract', 'Scrape Vanilla Beans', 'Vegetable Glycerin and Vanilla Beans', '@placeholder']\", where @placeholder shows the missing blank.\n",
    "\n",
    "There are 4 options given to choose for the blank:\n",
    "\n",
    "['How to Make Caffe Mocha', 'How to Make Koki Paratha', 'Vanilla Beans Can Use With Vegetable Glycerin', 'Prepare the Dough']\n",
    "\n",
    "We represent the question sequences using two custom identifiers '@q_pad' and '@c_pad'. So the above the question transforms to \n",
    "\n",
    "Ingredients Halal Vanilla Extract @q_pad Scrape Vanilla Beans @q_pad Vegetable Glycerin and Vanilla Beans @q_pad @placeholder @c_pad How to Make Caffe Mocha @c_pad How to Make Koki Paratha @c_pad Vanilla Beans Can Use With Vegetable Glycerin @c_pad Prepare the Dough\n",
    "\n",
    "Note: Since the question_text in all 'textual cloze' tasks is \n",
    "\"Choose the best title for the missing blank to correctly complete the recipe.\" we don't represent it in the question sequence.\n",
    "\n",
    "\n",
    "We model them as a Seq2seq task. The first RNN looks at the context sequence. The second RNN looks at the question sequence and we use the last hidden state of second RNN to classify to values [0,1,2,3] each representing choice numbers. It should give more than 25% accuracy(random guessing)\n",
    "\n",
    "Seq2Seq with Attention:\n",
    "Since the task is choosing a right option and not producing sequences. We will model attention in a different way for this task. Given the last question hidden state, we find attention the complete sequence of context+question and use this information in the final prediction linear layer.\n",
    "\n",
    "Observations:\n",
    "\n",
    "The train/test/valid accuracy is not getting improved. It is almost equivalent to random guessing. \n",
    "\n",
    "I suspect it is because the loss values aren't big enough to change parameters. \n",
    "\n",
    "When I run for a more number of epochs it is simply memorizing train set and not generalizing over test and valid data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eBHdr400RnrE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn3Rk83ESint"
   },
   "source": [
    "Download dataset( if not already downloaded )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TsH1jOKQTCXF"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "FILE=/content/train.json\n",
    "if [ ! -f \"$FILE\" ]; then\n",
    "  wget -c \"https://vision.cs.hacettepe.edu.tr/files/recipeqa/train.json\"\n",
    "fi\n",
    "file=/content/val.json\n",
    "if [ ! -f \"$file\" ]; then\n",
    "  wget -c \"https://vision.cs.hacettepe.edu.tr/files/recipeqa/val.json\"\n",
    "fi\n",
    "file=/content/test.json\n",
    "if [ ! -f \"$file\" ]; then\n",
    "  wget -c \"https://vision.cs.hacettepe.edu.tr/files/recipeqa/test.json\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZfccLlBZhGw"
   },
   "source": [
    "# Create Dataset and Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cr17PX1EZlos",
    "outputId": "f468f96f-3dc3-42ad-96d1-b66f82984a73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n",
      "100%|█████████▉| 398148/400000 [00:18<00:00, 21749.00it/s]"
     ]
    }
   ],
   "source": [
    "train_file = \"/content/train.json\"\n",
    "test_file = \"/content/test.json\"\n",
    "val_file = \"/content/val.json\"\n",
    "\n",
    "\n",
    "# Padding for dataset\n",
    "context_sequence_pad = ' @context '\n",
    "question_sequence_pad = ' @q_pad '\n",
    "choice_list_pad = ' @c_pad '\n",
    "\n",
    "# Selecting examples of 'textual cloze' and creating sequences with padding\n",
    "def get_examples(file):\n",
    "    ak = json.load(open(file))\n",
    "\n",
    "    k = [i for i in ak['data'] if i['task'] not in ['visual_coherence', 'visual_cloze', 'visual_ordering']]\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    for i, j in enumerate(k):\n",
    "        l = {}\n",
    "        l['context'] = context_sequence_pad.join([m['body'] for m in j['context']])\n",
    "        l['question'] = question_sequence_pad.join(j['question']) + choice_list_pad + choice_list_pad.join(\n",
    "            j['choice_list'])\n",
    "        l['answer'] = j['answer']\n",
    "        examples.append(l)\n",
    "    return examples\n",
    "\n",
    "train_examples = get_examples(train_file)\n",
    "test_examples = get_examples(test_file)\n",
    "val_examples = get_examples(val_file)\n",
    "\n",
    "# Defining fields for the context, question and answer\n",
    "context = data.Field(sequential=True, tokenize='spacy', init_token='<sos>', eos_token='<eos>')\n",
    "question = data.Field(sequential=True, tokenize='spacy', init_token='<sos>', eos_token='<eos>')\n",
    "answer = data.LabelField(is_target=True, preprocessing=lambda x: str(x), tokenize='spacy',sequential=False)\n",
    "fields = [('context', context), ('question', question), ('answer', answer)]\n",
    "\n",
    "# creating datasets\n",
    "train_Examples = [data.Example.fromlist([i['context'], i['question'], i['answer']], fields) for i in train_examples]\n",
    "train_dataset = data.Dataset(train_Examples, fields)\n",
    "test_Examples = [data.Example.fromlist([i['context'], i['question'], i['answer']], fields) for i in test_examples]\n",
    "test_dataset = data.Dataset(test_Examples, fields)\n",
    "val_Examples = [data.Example.fromlist([i['context'], i['question'], i['answer']], fields) for i in val_examples]\n",
    "val_dataset = data.Dataset(val_Examples, fields)\n",
    "\n",
    "\n",
    "#Build Vocabs\n",
    "context.build_vocab(train_dataset,min_freq = 2,max_size = 30000,vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "question.build_vocab(train_dataset, min_freq = 2,max_size = 6000,vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "answer.build_vocab(train_dataset)\n",
    "\n",
    "# build iterators\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, test_iterator, val_iterator = data.BucketIterator.splits((train_dataset, test_dataset, val_dataset),\n",
    "                                                                         batch_size=BATCH_SIZE,\n",
    "                                                                         sort_key=lambda x: len(x.context),\n",
    "                                                                         sort_within_batch=True,device= device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOyyKiSKhiAH",
    "outputId": "e694a7d3-211b-48ca-e655-5f9123e71302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': '3 until 5 whole vanilla beans250 gram of vegetable glycerin food gradeEvery 100 gram of vanilla beans have 35 until 40 of whole vanilla beans @context Scrape Vanilla Beans and get the seeds into vegetable glycerin @context Vanilla Beans Seed and Vegetable Glycerin @context Whole Vanilla Beans put in a bottle with seeds and vegetable glycerin', 'question': 'Ingredients Halal Vanilla Extract @q_pad Scrape Vanilla Beans @q_pad Vegetable Glycerin and Vanilla Beans @q_pad @placeholder @c_pad How to Make Caffe Mocha @c_pad How to Make Koki Paratha @c_pad Vanilla Beans Can Use With Vegetable Glycerin @c_pad Prepare the Dough', 'answer': 2}\n"
     ]
    }
   ],
   "source": [
    "print(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8t99xF4WwkgA",
    "outputId": "f68447e0-0be9-4e4a-c9cc-7a166bbd4e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7837\n"
     ]
    }
   ],
   "source": [
    "print(len(train_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7z1DzC1h6uY"
   },
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sq3G6hVhh_P3"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,context_dim,emb_dim,hid_dim,n_layers,dropout,bidirectional):\n",
    "    super().__init__()\n",
    "    self.hid_dim = hid_dim\n",
    "    \n",
    "    self.embedding = nn.Embedding(context_dim,emb_dim)\n",
    "\n",
    "    self.rnn = nn.LSTM(input_size=emb_dim,hidden_size = hid_dim,num_layers= n_layers,dropout= dropout,bidirectional = bidirectional)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self,context):\n",
    "    \n",
    "    embedded = self.dropout(self.embedding(context))\n",
    "\n",
    "    outputs, (hidden,cell_state) = self.rnn(embedded)\n",
    "\n",
    "    return outputs, hidden,cell_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "V0doD5B7lWxa"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self,question_dim,emb_dim,hid_dim,n_layers,bidirectional,dropout):\n",
    "    super().__init__()\n",
    "    self.hid_dim = hid_dim\n",
    "\n",
    "    self.embedded = nn.Embedding(question_dim,emb_dim)\n",
    "\n",
    "    self.rnn = nn.LSTM(input_size=emb_dim,hidden_size = hid_dim,num_layers= n_layers,dropout= dropout,bidirectional = bidirectional)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "\n",
    "  def forward(self, question, hidden, cell_state):\n",
    "    # since the task is not about sequence prediction but getting a representation of the 'question' sequence\n",
    "    # we just pass the sequence once and not step by step\n",
    "\n",
    "    input = self.dropout(self.embedded(question))\n",
    "\n",
    "    outputs , (hidden, cell_state) = self.rnn(input)\n",
    "\n",
    "    return outputs ,hidden,cell_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HFmNLiOxdeVA"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self,context_outputs, question_outputs, question_last_hidden):\n",
    "    # context_outputs : [context_src_len,batch_size,num_directions*hid_dim]\n",
    "    # question_outputs : [question_src_len,batch_size,num_directions*hid_dim]\n",
    "    # question_last_hidden : [1,batch_size,num_direction*hid_dim]\n",
    "\n",
    "    # concatenating context and question outputs\n",
    "    outputs = torch.cat([context_outputs,question_outputs],dim=0) # outputs : [context_src_len+question_src_len,batch_size,num_direction*hid_dim]\n",
    "\n",
    "    # Calculating alpha\n",
    "    \n",
    "    outputs = outputs.permute(1,0,2) # outputs :[batch_size,context_src_len+question_src_len,num_directions*hid_dim]\n",
    "    question_last_hidden_ = question_last_hidden.permute(1,2,0) # question_last_hidden_ = [batch_size,num_directions*hid_dim,1]\n",
    "\n",
    "    # Using dot product as the score function\n",
    "    temp = torch.bmm(outputs,question_last_hidden_) # temp = [batch_size,context_src_len+question_src_len,1]\n",
    "    alpha = temp.permute(0,2,1) # alpha : [batch_size,1,context_src_len+question_src_len]\n",
    "\n",
    "    # Calculating attention vector c\n",
    "    c_ = torch.bmm(alpha,outputs) # c_ : [batch_size,1,num_of_directions*hid_dim]\n",
    "    c = c_.permute(1,0,2) # c : [1,batch_size,num_of_directions*hid_dim] \n",
    "\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gJrDv3whozOr"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self,context_dim,question_dim,answer_dim,emb_dim,hid_dim,n_layers,bidirectional,dropout):\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = Encoder(context_dim,emb_dim,hid_dim,n_layers,dropout,bidirectional)\n",
    "\n",
    "    self.decoder = Decoder(question_dim,emb_dim,hid_dim,n_layers,bidirectional,dropout)\n",
    "\n",
    "    self.no_of_directions= 2 if bidirectional else 1\n",
    "\n",
    "    self.fc_out = nn.Linear(self.no_of_directions*hid_dim*2,answer_dim)\n",
    "\n",
    "    self.attention = Attention()\n",
    "\n",
    "  def forward(self,context,question):\n",
    "\n",
    "    encoder_outputs,encoder_hidden,encoder_cell_state = self.encoder(context)\n",
    "\n",
    "    decoder_outputs,hidden,cell_state = self.decoder(question,encoder_hidden,encoder_cell_state)\n",
    "\n",
    "    decoder_last_hidden = decoder_outputs[-1].unsqueeze(0)\n",
    "    \n",
    "    attention_vector = self.attention(encoder_outputs,decoder_outputs,decoder_last_hidden)\n",
    "\n",
    "    info_vector = torch.cat([decoder_last_hidden,attention_vector],dim=-1).squeeze(0)\n",
    "    \n",
    "    output = self.fc_out(info_vector)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhfvKb3LvuT-"
   },
   "source": [
    "# Initialize model, optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7y3nI2rHv14M",
    "outputId": "1deb5f17-af92-4951-ac5f-7bb9462581ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30004\n",
      "6004\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Embedding\n",
    "context_dim = len(context.vocab)\n",
    "question_dim = len(question.vocab)\n",
    "answer_dim = len(answer.vocab)\n",
    "\n",
    "print(context_dim)\n",
    "print(question_dim)\n",
    "print(answer_dim)\n",
    "\n",
    "emb_dim = 100\n",
    "hid_dim = 100\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "model = Seq2Seq(context_dim,question_dim,answer_dim,emb_dim,hid_dim,n_layers,bidirectional,dropout).to(device)\n",
    "\n",
    "context_pretrained_embeddings = context.vocab.vectors\n",
    "question_pretrained_embeddings = question.vocab.vectors\n",
    "\n",
    "model.encoder.embedding.weight.data = context_pretrained_embeddings.cuda()\n",
    "model.decoder.embedded.weight.data = question_pretrained_embeddings.cuda()\n",
    "\n",
    "def init_weights(m):\n",
    "  if not isinstance(m,Embedding):\n",
    "    for name,param in m.named_parameters():\n",
    "      nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(predictions, answers):\n",
    "  _, predictions = torch.max(predictions,1)\n",
    "  correct = (predictions == answers).float()\n",
    "  acc = correct.sum()/len(correct)\n",
    "\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGGZoudhyymE"
   },
   "source": [
    "# Train and Eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7ioXQcB2y1ku"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator,optimizer,criterion,clip):\n",
    "  model.train()\n",
    "\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "\n",
    "  for i, batch in enumerate(iterator):\n",
    "    context = batch.context\n",
    "    question = batch.question\n",
    "    answer = batch.answer\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(context,question)\n",
    "    \n",
    "    # answer = answer.t().squeeze()\n",
    "\n",
    "    loss = criterion(output,answer)\n",
    "    acc_ = accuracy(output,answer)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # torch.nn.utils.clip_grad_norm(model.parameters(),clip)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_acc += acc_.item()\n",
    "\n",
    "  return epoch_loss/len(iterator) , epoch_acc/len(iterator)\n",
    "\n",
    "def evaluate(model,iterator,criterion):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i,batch in enumerate(iterator):\n",
    "      context = batch.context\n",
    "      question = batch.question\n",
    "      answer = batch.answer\n",
    "\n",
    "      output = model(context,question)\n",
    "\n",
    "      # answer = answer.t().squeeze()\n",
    "\n",
    "      loss = criterion(output,answer)\n",
    "      acc_ = accuracy(output,answer)\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc_.item()\n",
    "\n",
    "  return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
    "\n",
    "def epoch_time(start_time,end_time):\n",
    "  elapsed_time = end_time - start_time\n",
    "  elapsed_mins = int(elapsed_time/60)\n",
    "  elapsed_secs = int(elapsed_time - (elapsed_mins*60))\n",
    "  return elapsed_mins,elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zygx8pBp14bF"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VzvQjp_Q161u",
    "outputId": "1f199695-c7df-48e5-a105-84263856c91b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 8s\n",
      "\tTrain Loss: 1.671 | Train Accuracy: 25.730\n",
      "\t Val. Loss: 1.891 |  Val. Accuracy: 28.535\n",
      "\t Test. Loss: 2.146 |  Val. Accuracy: 26.156\n",
      "Epoch: 02 | Time: 0m 8s\n",
      "\tTrain Loss: 1.513 | Train Accuracy: 25.503\n",
      "\t Val. Loss: 2.961 |  Val. Accuracy: 27.949\n",
      "\t Test. Loss: 3.432 |  Val. Accuracy: 25.277\n",
      "Epoch: 03 | Time: 0m 8s\n",
      "\tTrain Loss: 2.426 | Train Accuracy: 25.767\n",
      "\t Val. Loss: 1.562 |  Val. Accuracy: 26.289\n",
      "\t Test. Loss: 1.714 |  Val. Accuracy: 23.714\n",
      "Epoch: 04 | Time: 0m 8s\n",
      "\tTrain Loss: 1.733 | Train Accuracy: 28.237\n",
      "\t Val. Loss: 1.402 |  Val. Accuracy: 27.855\n",
      "\t Test. Loss: 1.412 |  Val. Accuracy: 26.431\n",
      "Epoch: 05 | Time: 0m 8s\n",
      "\tTrain Loss: 1.445 | Train Accuracy: 28.950\n",
      "\t Val. Loss: 1.813 |  Val. Accuracy: 22.428\n",
      "\t Test. Loss: 1.583 |  Val. Accuracy: 24.061\n",
      "Epoch: 06 | Time: 0m 8s\n",
      "\tTrain Loss: 1.567 | Train Accuracy: 26.348\n",
      "\t Val. Loss: 1.765 |  Val. Accuracy: 27.461\n",
      "\t Test. Loss: 1.938 |  Val. Accuracy: 24.984\n",
      "Epoch: 07 | Time: 0m 8s\n",
      "\tTrain Loss: 1.697 | Train Accuracy: 29.308\n",
      "\t Val. Loss: 1.448 |  Val. Accuracy: 22.529\n",
      "\t Test. Loss: 1.410 |  Val. Accuracy: 22.703\n",
      "Epoch: 08 | Time: 0m 8s\n",
      "\tTrain Loss: 1.436 | Train Accuracy: 33.114\n",
      "\t Val. Loss: 1.430 |  Val. Accuracy: 26.780\n",
      "\t Test. Loss: 1.503 |  Val. Accuracy: 24.407\n",
      "Epoch: 09 | Time: 0m 8s\n",
      "\tTrain Loss: 1.572 | Train Accuracy: 31.501\n",
      "\t Val. Loss: 1.504 |  Val. Accuracy: 24.271\n",
      "\t Test. Loss: 1.438 |  Val. Accuracy: 24.079\n",
      "Epoch: 10 | Time: 0m 8s\n",
      "\tTrain Loss: 1.388 | Train Accuracy: 32.676\n",
      "\t Val. Loss: 1.593 |  Val. Accuracy: 22.822\n",
      "\t Test. Loss: 1.502 |  Val. Accuracy: 23.289\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  train_loss,train_acc = train(model,train_iterator,optimizer,criterion,CLIP)\n",
    "  test_loss,test_acc = evaluate(model,test_iterator,criterion)\n",
    "  val_loss,val_acc = evaluate(model,val_iterator,criterion)\n",
    "\n",
    "  end_time = time.time()\n",
    "\n",
    "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "  if val_loss < best_valid_loss:\n",
    "    best_valid_loss = val_loss\n",
    "    torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "  print(f'\\tTrain Loss: {train_loss:.3f} | Train Accuracy: {train_acc*100:.3f}')\n",
    "  print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Accuracy: {val_acc*100:.3f}')\n",
    "  print(f'\\t Test. Loss: {test_loss:.3f} |  Val. Accuracy: {test_acc*100:.3f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "RecipeQA with seq2seq with attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
